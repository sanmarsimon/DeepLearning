{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BY:\n",
        "\n",
        "Aymen_Zeghaida_1926415\n",
        "\n",
        "\n",
        "Simeon_Sanmar_1938126"
      ],
      "metadata": {
        "id": "9jyP9gb_xef5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmOFPR8VmUq"
      },
      "source": [
        "# Machine translation\n",
        "\n",
        "The goal of this TP is to build a machine translation model.\n",
        "You will be comparing the performance of three different architectures:\n",
        "* A vanilla RNN\n",
        "* A GRU-RNN\n",
        "* A transformer\n",
        "\n",
        "You are provided with the code to load and build the pytorch dataset,\n",
        "and the code for the training loop.\n",
        "You \"only\" have to code the architectures.\n",
        "Of course, the use of built-in torch layers such as `nn.GRU`, `nn.RNN` or `nn.Transformer`\n",
        "is forbidden, as there would be no exercise otherwise.\n",
        "\n",
        "The source sentences are in english and the target language is french.\n",
        "\n",
        "This is also for you the occasion to see what a basic machine learning pipeline looks like.\n",
        "Take a look at the given code, you might learn a lot!\n",
        "\n",
        "Do not forget to **select the runtime type as GPU!**\n",
        "\n",
        "**Sources**\n",
        "\n",
        "* Dataset: [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/)\n",
        "\n",
        "<!---\n",
        "M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proc. of EAMT, pp. 261-268, Trento, Italy. pdf, bib. [paper](https://aclanthology.org/2012.eamt-1.60.pdf). [website](https://wit3.fbk.eu/2016-01).\n",
        "-->\n",
        "\n",
        "* The code is inspired by this [pytorch tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html).\n",
        "\n",
        "*This notebook is quite big, use the table of contents to easily navigate through it.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyCdlapMV8Hu"
      },
      "source": [
        "# Imports and data initializations\n",
        "\n",
        "We first download and parse the dataset. From the parsed sentences\n",
        "we can build the vocabularies and the torch datasets.\n",
        "The end goal of this section is to have an iterator\n",
        "that can yield the pairs of translated datasets, and\n",
        "where each sentences is made of a sequence of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "vLbVbH4lu4J0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cJQfREvFUdoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf804f8a-1185-4e12-aa4a-af688a41bc2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-10 09:05:59.413434: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-04-10 09:05:59.483335: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-10 09:06:00.502070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-04-10 09:06:12.309865: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-04-10 09:06:12.364914: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-10 09:06:13.359388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'fr' are deprecated. Please use the\n",
            "full pipeline package name 'fr_core_news_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.5.0/fr_core_news_sm-3.5.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from fr-core-news-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->fr-core-news-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.9/dist-packages (1.7.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.14.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.19.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download en \n",
        "!python3 -m spacy download fr \n",
        "!pip install torchinfo \n",
        "!pip install einops \n",
        "!pip install wandb \n",
        "\n",
        "\n",
        "from itertools import takewhile\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torchtext.datasets import IWSLT2016\n",
        "\n",
        "import einops\n",
        "import wandb\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizers are objects that are able to divide a python string into a list of tokens (words, punctuations, special tokens...) as a list of strings.\n",
        "\n",
        "The special tokens are used for a particular reasons:\n",
        "* *\\<unk\\>*: Replace an unknown word in the vocabulary by this default token\n",
        "* *\\<pad\\>*: Virtual token used to as padding token so a batch of sentences can have a unique length\n",
        "* *\\<bos\\>*: Token indicating the beggining of a sentence in the target sequence\n",
        "* *\\<eos\\>*: Token indicating the end of a sentence in the target sequence"
      ],
      "metadata": {
        "id": "ppPj9CrnsSoW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DxNpMbkvUfGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbe6aad-c350-42d1-ac94-d53ebb60e84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-10 09:06:36--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7420323 (7.1M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip.2’\n",
            "\n",
            "fra-eng.zip.2       100%[===================>]   7.08M  6.00MB/s    in 1.2s    \n",
            "\n",
            "2023-04-10 09:06:38 (6.00 MB/s) - ‘fra-eng.zip.2’ saved [7420323/7420323]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "196177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"fr\" could not be loaded, trying \"fr_core_news_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Original dataset, but there's a bug on Colab with it\n",
        "# train, valid, _ = IWSLT2016(language_pair=('fr', 'en'))\n",
        "# train, valid = list(train), list(valid)\n",
        "\n",
        "# Another dataset, but it is too huge\n",
        "# !wget https://www.statmt.org/wmt14/training-monolingual-europarl-v7/europarl-v7.en.gz\n",
        "# !wget https://www.statmt.org/wmt14/training-monolingual-europarl-v7/europarl-v7.fr.gz\n",
        "# !gunzip europarl-v7.en.gz\n",
        "# !gunzip europarl-v7.fr.gz\n",
        "\n",
        "# with open('europarl-v7.en', 'r') as my_file:\n",
        "#     english = my_file.readlines()\n",
        "\n",
        "# with open('europarl-v7.fr', 'r') as my_file:\n",
        "#     french = my_file.readlines()\n",
        "\n",
        "# dataset = [\n",
        "#     (en, fr)\n",
        "#     for en, fr in zip(english, french)\n",
        "# ]\n",
        "# print(f'\\n{len(dataset):,} sentences.')\n",
        "\n",
        "# dataset, _ = train_test_split(dataset, test_size=0.8, random_state=0)  # Remove 80% of the dataset (it would be huge otherwise)\n",
        "# train, valid = train_test_split(dataset, test_size=0.2, random_state=0)  # Split between train and validation dataset\n",
        "\n",
        "# Our current dataset\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "\n",
        "\n",
        "df = pd.read_csv('fra.txt', sep='\\t', names=['english', 'french', 'attribution'])\n",
        "train = [\n",
        "    (en, fr) for en, fr in zip(df['english'], df['french'])\n",
        "]\n",
        "train, valid = train_test_split(train, test_size=0.1, random_state=0)\n",
        "print(len(train))\n",
        "\n",
        "en_tokenizer, fr_tokenizer = get_tokenizer('spacy', language='en'), get_tokenizer('spacy', language='fr')\n",
        "\n",
        "SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ddZvN5FiK9u"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Functions and classes to build the vocabularies and the torch datasets.\n",
        "The vocabulary is an object able to transform a string token into the id (an int) of that token in the vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z2dKQ6PvZC_U"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset: list,\n",
        "            en_vocab: Vocab,\n",
        "            fr_vocab: Vocab,\n",
        "            en_tokenizer,\n",
        "            fr_tokenizer,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.en_vocab = en_vocab\n",
        "        self.fr_vocab = fr_vocab\n",
        "        self.en_tokenizer = en_tokenizer\n",
        "        self.fr_tokenizer = fr_tokenizer\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of examples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple:\n",
        "        \"\"\"Return a sample.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            index: Index of the sample.\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            en_tokens: English tokens of the sample, as a LongTensor.\n",
        "            fr_tokens: French tokens of the sample, as a LongTensor.\n",
        "        \"\"\"\n",
        "        # Get the strings\n",
        "        en_sentence, fr_sentence = self.dataset[index]\n",
        "\n",
        "        # To list of words\n",
        "        # We also add the beggining-of-sentence and end-of-sentence tokens\n",
        "        en_tokens = ['<bos>'] + self.en_tokenizer(en_sentence) + ['<eos>']\n",
        "        fr_tokens = ['<bos>'] + self.fr_tokenizer(fr_sentence) + ['<eos>']\n",
        "\n",
        "        # To list of tokens\n",
        "        en_tokens = self.en_vocab(en_tokens)  # list[int]\n",
        "        fr_tokens = self.fr_vocab(fr_tokens)\n",
        "\n",
        "        return torch.LongTensor(en_tokens), torch.LongTensor(fr_tokens)\n",
        "\n",
        "\n",
        "def yield_tokens(dataset, tokenizer, lang):\n",
        "    \"\"\"Tokenize the whole dataset and yield the tokens.\n",
        "    \"\"\"\n",
        "    assert lang in ('en', 'fr')\n",
        "    sentence_idx = 0 if lang == 'en' else 1\n",
        "\n",
        "    for sentences in dataset:\n",
        "        sentence = sentences[sentence_idx]\n",
        "        tokens = tokenizer(sentence)\n",
        "        yield tokens\n",
        "\n",
        "\n",
        "def build_vocab(dataset: list, en_tokenizer, fr_tokenizer, min_freq: int):\n",
        "    \"\"\"Return two vocabularies, one for each language.\n",
        "    \"\"\"\n",
        "    en_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, en_tokenizer, 'en'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    en_vocab.set_default_index(en_vocab['<unk>'])  # Default token for unknown words\n",
        "\n",
        "    fr_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, fr_tokenizer, 'fr'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    fr_vocab.set_default_index(fr_vocab['<unk>'])\n",
        "\n",
        "    return en_vocab, fr_vocab\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "        dataset: list,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        max_words: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Preprocess the dataset.\n",
        "    Remove samples where at least one of the sentences are too long.\n",
        "    Those samples takes too much memory.\n",
        "    Also remove the pending '\\n' at the end of sentences.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "\n",
        "    for en_s, fr_s in dataset:\n",
        "        if len(en_tokenizer(en_s)) >= max_words or len(fr_tokenizer(fr_s)) >= max_words:\n",
        "            continue\n",
        "        \n",
        "        en_s = en_s.replace('\\n', '')\n",
        "        fr_s = fr_s.replace('\\n', '')\n",
        "\n",
        "        filtered.append((en_s, fr_s))\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def build_datasets(\n",
        "        max_sequence_length: int,\n",
        "        min_token_freq: int,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        train: list,\n",
        "        val: list,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Build the training, validation and testing datasets.\n",
        "    It takes care of the vocabulary creation.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        - max_sequence_length: Maximum number of tokens in each sequences.\n",
        "            Having big sequences increases dramatically the VRAM taken during training.\n",
        "        - min_token_freq: Minimum number of occurences each token must have\n",
        "            to be saved in the vocabulary. Reducing this number increases\n",
        "            the vocabularies's size.\n",
        "        - en_tokenizer: Tokenizer for the english sentences.\n",
        "        - fr_tokenizer: Tokenizer for the french sentences.\n",
        "        - train and val: List containing the pairs (english, french) sentences.\n",
        "\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        - (train_dataset, val_dataset): Tuple of the two TranslationDataset objects.\n",
        "    \"\"\"\n",
        "    datasets = [\n",
        "        preprocess(samples, en_tokenizer, fr_tokenizer, max_sequence_length)\n",
        "        for samples in [train, val]\n",
        "    ]\n",
        "\n",
        "    en_vocab, fr_vocab = build_vocab(datasets[0], en_tokenizer, fr_tokenizer, min_token_freq)\n",
        "\n",
        "    datasets = [\n",
        "        TranslationDataset(samples, en_vocab, fr_vocab, en_tokenizer, fr_tokenizer)\n",
        "        for samples in datasets\n",
        "    ]\n",
        "\n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GWlH-qEbkoYA"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch: list, src_pad_idx: int, tgt_pad_idx: int) -> tuple:\n",
        "    \"\"\"Add padding to the given batch so that all\n",
        "    the samples are of the same size.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        data_batch: List of samples.\n",
        "            Each sample is a tuple of LongTensors of varying size.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "    \n",
        "    Output\n",
        "    ------\n",
        "        en_batch: Batch of tokens for the padded english sentences.\n",
        "            Shape of [batch_size, max_en_len].\n",
        "        fr_batch: Batch of tokens for the padded french sentences.\n",
        "            Shape of [batch_size, max_fr_len].\n",
        "    \"\"\"\n",
        "    en_batch, fr_batch = [], []\n",
        "    for en_tokens, fr_tokens in data_batch:\n",
        "        en_batch.append(en_tokens)\n",
        "        fr_batch.append(fr_tokens)\n",
        "\n",
        "    en_batch = pad_sequence(en_batch, padding_value=src_pad_idx, batch_first=True)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=tgt_pad_idx, batch_first=True)\n",
        "    return en_batch, fr_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Gs4Myjh-jV"
      },
      "source": [
        "# Models architecture\n",
        "This is where you have to code the architectures.\n",
        "\n",
        "In a machine translation task, the model takes as input the whole\n",
        "source sentence along with the current known tokens of the target,\n",
        "and predict the next token in the target sequence.\n",
        "This means that the target tokens are predicted in an autoregressive\n",
        "manner, starting from the first token (right after the *\\<bos\\>* token) and producing tokens one by one until the last *\\<eos\\>* token.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "The loss is simply a *cross entropy loss* over the whole steps, where each class is a token of the vocabulary.\n",
        "\n",
        "![RNN schema for machinea translation](https://www.simplilearn.com/ice9/free_resources_article_thumb/machine-translation-model-with-encoder-decoder-rnn.jpg)\n",
        "\n",
        "Note that in this image the english sentence is provided in reverse. \n",
        "\n",
        "---\n",
        "\n",
        "In pytorch, there is no dinstinction between an intermediate layer or a whole model having multiple layers in itself.\n",
        "Every layers or models inherit from the `torch.nn.Module`.\n",
        "This module needs to define the `__init__` method where you instanciate the layers,\n",
        "and the `forward` method where you decide how the inputs and the layers of the module interact between them.\n",
        "Thanks to the autograd computations of pytorch, you do not have\n",
        "to implement any backward method!\n",
        "\n",
        "A really important advice is to **always look at\n",
        "the shape of your input and your output.**\n",
        "From that, you can often guess how the layers should interact\n",
        "with the inputs to produce the right output.\n",
        "You can also easily detect if there's something wrong going on.\n",
        "\n",
        "You are more than advised to use the `einops` library and the `torch.einsum` function. This will require less operations than 'classical' code, but note that it's a bit trickier to use.\n",
        "This is a way of describing tensors manipulation with strings, bypassing the multiple tensor methods executed in the background.\n",
        "You can find a nice presentation of `einops` [here](https://einops.rocks/1-einops-basics/).\n",
        "A paper has just been released about einops [here](https://paperswithcode.com/paper/einops-clear-and-reliable-tensor).\n",
        "\n",
        "**A great tutorial on pytorch can be found [here](https://stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html).**\n",
        "Spending 3 hours on this tutorial is *no* waste of time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN models"
      ],
      "metadata": {
        "id": "xodRThXg2DHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN\n",
        "Here you have to implement a recurrent neural network. You will need to create a single RNN Layer, and a module allowing to stack these layers. Look up the pytorch documentation to figure out this module's operations and what is communicated from one layer to another.\n",
        "\n",
        "The `RNNCell` layer produce one hidden state vector for each sentence in the batch\n",
        "(useful for the output of the encoder), and also produce one embedding for each\n",
        "token in each sentence (useful for the output of the decoder).\n",
        "\n",
        "The `RNN` module is composed of a stack of `RNNCell`. Each token embeddings\n",
        "coming out from a previous `RNNCell` is used as an input for the next `RNNCell` layer.\n",
        "\n",
        "**Be careful !** Our `RNNCell` implementation is not exactly the same thing as\n",
        "the PyTorch's `nn.RNNCell`. PyTorch implements only the operations for one token\n",
        "(so you would need to loop through each tokens inside the `RNN` instead).\n",
        "You are free to implement `RNN` and `RNNCell` the way you want, as long as it has the expected behaviour of a RNN.\n",
        "\n",
        "The same thing apply for the `GRU` and `GRUCell`.\n"
      ],
      "metadata": {
        "id": "ZvfRVUKm1u8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell(nn.Module):\n",
        "    \"\"\"A single RNN layer.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "\n",
        "    Important note: This layer does not exactly the same thing as nn.RNNCell does.\n",
        "    PyTorch implementation is only doing one simple pass over one token for each batch.\n",
        "    This implementation is taking the whole sequence of each batch and provide the\n",
        "    final hidden state along with the embeddings of each token in each sequence.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.Wih = nn.Linear(input_size, hidden_size)\n",
        "        # TODO\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Linear transformation from hidden state to hidden state\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = config[\"device\"]\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"Go through all the sequence in x, iteratively updating\n",
        "        the hidden state h.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Create an empty tensor for the token embeddings y\n",
        "        y = torch.empty(batch_size, seq_len, self.hidden_size).to(self.device)\n",
        "\n",
        "        # Iterate over the sequence in x\n",
        "        for sequence in range(seq_len):\n",
        "            # Linear transformation of the current input token\n",
        "            # dims : batch_size x hidden_size \n",
        "            # Wih * x + bih\n",
        "            ih = self.Wih(x[:, sequence]) \n",
        "            \n",
        "            # Linear transformation of the previous hidden state\n",
        "            # Whh*h + bhh\n",
        "            h = self.Whh(h)  \n",
        "            \n",
        "            # Apply the activation function to the sum of the two linear transformations\n",
        "            h = self.tanh(h + ih) # [batch_size, hidden_size]\n",
        "            \n",
        "            # Store the current hidden state as the token embedding for the current position in the sequence\n",
        "            # dims batch_size x seq_len x hidden_size]\n",
        "            y[:, sequence] = h           \n",
        "        \n",
        "        h = self.dropout(h)\n",
        "        y = self.dropout(y)\n",
        "        return y, h\n",
        "\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"Implementation of an RNN based\n",
        "    on https://pytorch.org/docs/stable/generated/torch.nn.RNN.html.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        num_layers: Number of layers (RNNCell or GRUCell).\n",
        "        dropout: Dropout rate.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "            This parameter can be removed if you decide to use the module `GRU`.\n",
        "            Indeed, `GRU` should have exactly the same code as this module,\n",
        "            but with `GRUCell` instead of `RNNCell`. We let the freedom for you\n",
        "            to decide at which level you want to specialise the modules (either\n",
        "            in `TranslationRNN` by creating a `GRU` or a `RNN`, or in `RNN`\n",
        "            by creating a `GRUCell` or a `RNNCell`).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            num_layers: int,\n",
        "            dropout: float,\n",
        "            model_type: str,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Linear transformation from hidden state to hidden state\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = config[\"device\"]\n",
        "\n",
        "        # Create the RNN layers\n",
        "        self.model = nn.ModuleList([\n",
        "            RNNCell(\n",
        "                input_size if layer == 0 else hidden_size, \n",
        "                hidden_size, \n",
        "                dropout if layer != num_layers - 1 else 0\n",
        "            ) if model_type == 'RNN' else \n",
        "            GRUCell(\n",
        "                input_size if layer == 0 else hidden_size, \n",
        "                hidden_size, \n",
        "                dropout if layer != num_layers - 1 else 0\n",
        "            ) for layer in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor=None) -> tuple:\n",
        "        \"\"\"Pass the input sequence through all the RNN cells.\n",
        "        Returns the output and the final hidden state of each RNN layer\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Hidden state for each RNN layer.\n",
        "                Can be None, in which case an initial hidden state is created.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Output embeddings for each token after the RNN layers.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Final hidden state.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "        \"\"\"\n",
        "        batch_size,_,_ = x.size()\n",
        "        \n",
        "        # Output embeddings for each token after the RNN layers\n",
        "        output_emb = x\n",
        "        \n",
        "        # Final hidden state for each RNN layer\n",
        "        final_hidden_state = torch.empty(batch_size, self.num_layers, self.hidden_size).to(self.device)\n",
        "        \n",
        "        if h == None:\n",
        "            h = torch.zeros(batch_size, self.num_layers, self.hidden_size).to(self.device)\n",
        "\n",
        "        \n",
        "        # Iterate over all the RNN layers\n",
        "        # l is the layer\n",
        "        for l, cell in enumerate(self.model):\n",
        "            # Select the current hidden state for this RNN layer\n",
        "            hh = h[:, l]        \n",
        "            # Apply the RNN cell to the input and the hidden state           \n",
        "            output_emb, hh = cell(output_emb, hh) # in/out ([batch_size, seq_len, hidden_size], [batch_size, hidden_size]).\n",
        "            # Save the hidden state for this RNN layer\n",
        "            final_hidden_state[:, l] = hh                \n",
        "        return output_emb, final_hidden_state"
      ],
      "metadata": {
        "id": "RiNKnwScM5Tc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU\n",
        "Here you have to implement a GRU-RNN. This architecture is close to the Vanilla RNN but perform different operations. Look up the pytorch documentation to figure out the differences."
      ],
      "metadata": {
        "id": "I0ciaamtvK0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    \"\"\"Implementation of a GRU based on https://pytorch.org/docs/stable/generated/torch.nn.GRU.html.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        num_layers: Number of layers.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            num_layers: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.device = config[\"device\"]\n",
        "  \n",
        "        self.model = nn.ModuleList([GRUCell(\n",
        "            input_size if n == 0 else hidden_size, \n",
        "            hidden_size, \n",
        "            dropout if n != (num_layers-1) else 0) for n in range(num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor=None) -> tuple:\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state for each layer.\n",
        "                If 'None', then an initial hidden state (a zero filled tensor)\n",
        "                is created.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            output:\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h_n: Final hidden state.\n",
        "                Shape of [batch_size, n_layers, hidden size].\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # pass\n",
        "        batch_size,_,_ = x.size()\n",
        "\n",
        "        # Output embeddings for each token after the RNN layers\n",
        "        output_emb = x\n",
        "        \n",
        "        # Final hidden state for each RNN layer\n",
        "        final_hidden_state = torch.empty(batch_size, self.num_layers, self.hidden_size).to(self.device)\n",
        "\n",
        "        if h == None:\n",
        "            h = torch.zeros(batch_size, self.num_layers, self.hidden_size).to(self.device)\n",
        "        \n",
        "        # Iterate over all the RNN layers\n",
        "        # l is the layer\n",
        "        for l, cell in enumerate(self.model):\n",
        "            hh = h[:, l]                \n",
        "            output_emb, hh = cell(output_emb, hh)         \n",
        "            final_hidden_state[:, l] = hh            \n",
        "        return output_emb, final_hidden_state\n",
        "\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "    \"\"\"A single GRU layer.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        # TODO\n",
        "        self.gate_size = 3\n",
        "\n",
        "        self.Wih = nn.Linear(input_size, hidden_size * self.gate_size)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size * self.gate_size)\n",
        "\n",
        "        self.device = config[\"device\"]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        \n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-std, std)\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        \n",
        "        # Output embeddings for each token after the GRU layer\n",
        "        output_emb = torch.empty(batch_size, seq_len, self.hidden_size).to(self.device)\n",
        "        \n",
        "        # Last hidden state\n",
        "        final_hidden_state = h\n",
        "\n",
        "        # Iterate over all the tokens in the input sequence\n",
        "        for token in range(seq_len):\n",
        "            # Apply the GRU cell to the current input token and hidden state\n",
        "            # Wih * input + bih\n",
        "            x_t = self.Wih(x[:, token]) \n",
        "            # Whh * hidden_state_t-1 + bhh\n",
        "            hidden_state_t = self.Whh(final_hidden_state) # [batch_size, hidden_size]\n",
        "\n",
        "            # Split the input and hidden state into the reset, update, and new gates\n",
        "            # and compute the gate activations\n",
        "            i_reset, i_update, i_new = x_t.chunk(self.gate_size, 1) \n",
        "            h_reset, h_update, h_new = hidden_state_t.chunk(self.gate_size, 1) \n",
        "\n",
        "            reset_gate = torch.sigmoid(i_reset + h_reset)\n",
        "            update_gate = torch.sigmoid(i_update + h_update)\n",
        "            new_candidate_state = torch.tanh(i_new + (reset_gate * h_new))\n",
        "\n",
        "            final_hidden_state = update_gate * final_hidden_state + (1 - update_gate) * new_candidate_state  \n",
        "            \n",
        "            output_emb[:, token] = final_hidden_state # [batch_size, seq_len, hidden_size]\n",
        "        \n",
        "        output_emb = self.dropout(output_emb)\n",
        "        final_hidden_state = self.dropout(final_hidden_state)\n",
        "\n",
        "        return output_emb, final_hidden_state"
      ],
      "metadata": {
        "id": "xdAMSZ7EMrMN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation RNN\n",
        "\n",
        "This module instanciates a vanilla RNN or a GRU-RNN and performs the translation task. You have to:\n",
        "* Encode the source and target sequence\n",
        "* Pass the final hidden state of the encoder to the decoder (one for each layer)\n",
        "* Decode the hidden state into the target sequence\n",
        "\n",
        "We use teacher forcing for training, meaning that when the next token is predicted, that prediction is based on the previous true target tokens. "
      ],
      "metadata": {
        "id": "boIetZUy1f-5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KD-6N17xhuLy"
      },
      "outputs": [],
      "source": [
        "class TranslationRNN(nn.Module):\n",
        "    \"\"\"Basic RNN encoder and decoder for a translation task.\n",
        "    It can run as a vanilla RNN or a GRU-RNN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the hidden layers in the RNNs\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the RNNs.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "            model_type: str,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.device = config['device']\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding_src = nn.Embedding(num_embeddings=n_tokens_src, embedding_dim=dim_embedding, padding_idx=src_pad_idx)\n",
        "        self.embedding_tgt = nn.Embedding(num_embeddings=n_tokens_tgt, embedding_dim=dim_embedding, padding_idx=tgt_pad_idx)\n",
        "\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "\n",
        "        if model_type == 'RNN':\n",
        "            self.encoder = RNN(input_size=dim_embedding, \n",
        "                            hidden_size=dim_hidden, \n",
        "                            num_layers=n_layers, \n",
        "                            dropout=dropout, \n",
        "                            model_type=model_type)\n",
        "            self.decoder = RNN(input_size=dim_embedding, \n",
        "                            hidden_size=dim_hidden, \n",
        "                            num_layers=n_layers, \n",
        "                            dropout=dropout, \n",
        "                            model_type=model_type)\n",
        "            \n",
        "        elif model_type == 'GRU':\n",
        "            self.encoder = GRU(input_size=dim_embedding, \n",
        "                            hidden_size=dim_hidden, \n",
        "                            num_layers=n_layers, \n",
        "                            dropout=dropout)\n",
        "            self.decoder = GRU(input_size=dim_embedding, \n",
        "                            hidden_size=dim_hidden, \n",
        "                            num_layers=n_layers, \n",
        "                            dropout=dropout)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(dim_hidden)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim_hidden, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, n_tokens_tgt)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens logites based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "        \n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, tgt_seq_len, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "\n",
        "        # Embed the source sentence\n",
        "        x_source = self.embedding_src(source)  \n",
        "\n",
        "        # Encode the source sentence\n",
        "        _, h_enc = self.encoder(x_source)       \n",
        "        \n",
        "        # Embed the target sentence\n",
        "        x_target = self.embedding_tgt(target)  \n",
        "        \n",
        "        # Normalize the encoded hidden state\n",
        "        h_enc = self.layer_norm(h_enc)\n",
        "        \n",
        "        # Decode the target sentence\n",
        "        x_dec, _ = self.decoder(x_target, h_enc) \n",
        "        \n",
        "        # Pass the decoded target sentence through the MLP to get logits\n",
        "        y = self.mlp(x_dec) \n",
        "        \n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer model\n",
        "Here you have to code the Transformer architecture.\n",
        "It is divided in three parts:\n",
        "* Attention layers\n",
        "* Encoder and decoder layers\n",
        "* Main layers (gather the encoder and decoder layers)\n",
        "\n",
        "The [illustrated transformer](https://jalammar.github.io/illustrated-transformer/) blog can help you\n",
        "understanding how the architecture works.\n",
        "Once this is done, you can use [the annontated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) to have an idea of how to code this architecture.\n",
        "We encourage you to use `torch.einsum` and the `einops` library as much as you can. It will make your code simpler.\n",
        "\n",
        "---\n",
        "**Implementation order**\n",
        "\n",
        "To help you with the implementation, we advise you following this order:\n",
        "* Implement `TranslationTransformer` and use `nn.Transformer` instead of `Transformer`\n",
        "* Implement `Transformer` and use `nn.TransformerDecoder` and `nn.TransformerEnocder`\n",
        "* Implement the `TransformerDecoder` and `TransformerEncoder` and use `nn.MultiHeadAttention`\n",
        "* Implement `MultiHeadAttention`\n",
        "\n",
        "Do not forget to add `batch_first=True` when necessary in the `nn` modules."
      ],
      "metadata": {
        "id": "EZcGlRnZvOnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention layers\n",
        "We use a `MultiHeadAttention` module, that is able to perform self-attention aswell as cross-attention (depending on what you give as queries, keys and values).\n",
        "\n",
        "**Attention**\n",
        "\n",
        "\n",
        "It takes the multiheaded queries, keys and values as input.\n",
        "It computes the attention between the queries and the keys and return the attended values.\n",
        "\n",
        "The implementation of this function can greatly be improved with *einsums*.\n",
        "\n",
        "**MultiheadAttention**\n",
        "\n",
        "Computes the multihead queries, keys and values and feed them to the `attention` function.\n",
        "You also need to merge the key padding mask and the attention mask into one mask.\n",
        "\n",
        "The implementation of this module can greatly be improved with *einops.rearrange*."
      ],
      "metadata": {
        "id": "OFxV-6M3402p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "def attention(\n",
        "        q: torch.FloatTensor,\n",
        "        k: torch.FloatTensor,\n",
        "        v: torch.FloatTensor,\n",
        "        mask: torch.BoolTensor=None,\n",
        "        dropout: nn.Dropout=None,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Computes multihead scaled dot-product attention from the\n",
        "    projected queries, keys and values.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        q: Batch of queries.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        k: Batch of keys.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        v: Batch of values.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        mask: Prevent tokens to attend to some other tokens (for padding or autoregressive attention).\n",
        "            Attention is prevented where the mask is `True`.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2],\n",
        "            or broadcastable to that shape.\n",
        "        dropout: Dropout layer to use.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        y: Multihead scaled dot-attention between the queries, keys and values.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        attn: Computed attention between the keys and the queries.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2].\n",
        "    \"\"\"\n",
        "    # Compute the scaling factor for the dot product\n",
        "    scaling_factor = torch.tensor(q.size(-1))\n",
        "\n",
        "    # Compute the dot product between queries and keys, and scale it down by the square root of the dimension size\n",
        "    attn = torch.einsum('bshd,bihd->bhsi', q, k) / torch.sqrt(scaling_factor) \n",
        "\n",
        "    # Apply the mask to prevent attention to certain tokens (e.g. for padding or autoregressive attention)\n",
        "    if mask is not None:\n",
        "        attn = attn.masked_fill(mask, float(\"-1e20\")) \n",
        "    #  for numerical stability\n",
        "\n",
        "    # Compute the softmax of the dot product to get the attention weights\n",
        "    attn_out = torch.softmax(attn, dim = -1) \n",
        "    if dropout is not None:\n",
        "        attn_out = dropout(attn_out)\n",
        "        \n",
        "    # Compute the weighted sum of values using the attention weights to get the final output\n",
        "    y = torch.einsum('bhst,bthd->bshd', attn_out, v)\n",
        "\n",
        "    return y, attn_out\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multihead attention module.\n",
        "    Can be used as a self-attention and cross-attention layer.\n",
        "    The queries, keys and values are projected into multiple heads\n",
        "    before computing the attention between those tensors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        dim: Dimension of the input tokens.\n",
        "        n_heads: Number of heads. `dim` must be divisible by `n_heads`.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim: int,\n",
        "            n_heads: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert dim % n_heads == 0\n",
        "\n",
        "        # TODO\n",
        "        self.n_heads = n_heads\n",
        "        self.dim = dim        \n",
        "        self.attention = attention\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.q_lin  = nn.Linear(dim, dim)\n",
        "        self.v_lin  = nn.Linear(dim, dim)\n",
        "        self.k_lin  = nn.Linear(dim, dim)\n",
        "        self.linear = nn.Linear(dim, dim)    \n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            q: torch.FloatTensor,\n",
        "            k: torch.FloatTensor,\n",
        "            v: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor = None,\n",
        "            attn_mask: torch.BoolTensor = None,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Computes the scaled multi-head attention form the input queries,\n",
        "        keys and values.\n",
        "\n",
        "        Project those queries, keys and values before feeding them\n",
        "        to the `attention` function.\n",
        "\n",
        "        The masks are boolean masks. Tokens are prevented to attends to\n",
        "        positions where the mask is `True`.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            q: Batch of queries.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "            k: Batch of keys.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            v: Batch of values.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            key_padding_mask: Prevent attending to padding tokens.\n",
        "                Shape of [batch_size, seq_len_2].\n",
        "            attn_mask: Prevent attending to subsequent tokens.\n",
        "                Shape of [seq_len_1, seq_len_2].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Computed multihead attention.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "        \"\"\"\n",
        "        batch,seq_len_q,_ = q.size() # query sequence length\n",
        "        seq_len_k         = k.size(1) # key sequence length\n",
        "        seq_len_v         = v.size(1) # value sequence length\n",
        "        \n",
        "        q = self.q_lin(q) \n",
        "        k = self.k_lin(k)\n",
        "        v = self.v_lin(v)\n",
        "\n",
        "        # Project queries, keys, and values into multiple heads\n",
        "        q = q.reshape(batch, seq_len_q, self.n_heads, self.dim // self.n_heads) \n",
        "\n",
        "        k = k.reshape(batch, seq_len_k, self.n_heads, self.dim // self.n_heads) \n",
        "\n",
        "        v = v.reshape(batch, seq_len_v, self.n_heads, self.dim // self.n_heads) \n",
        "\n",
        "        # Merge key_padding mask with attn_mask\n",
        "        key_padding_mask = key_padding_mask.view(batch, 1, 1, seq_len_k).expand(-1, self.n_heads, -1, -1).reshape(batch * self.n_heads, 1, seq_len_k)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.logical_or(key_padding_mask) \n",
        "            attn_mask = attn_mask.reshape(batch, self.n_heads, seq_len_q, seq_len_q)\n",
        "        \n",
        "        # Compute attention\n",
        "        attn, _ = self.attention(q, k, v, attn_mask, self.dropout) \n",
        "        # Merge heads\n",
        "        attn = rearrange(attn, 'b s h e -> b s (h e)') \n",
        "        # Linear projection for output\n",
        "        attn_out = self.linear(attn)\n",
        "        \n",
        "        return attn_out\n",
        "\n"
      ],
      "metadata": {
        "id": "A0jOZxOwu_Uj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder and decoder layers\n",
        "\n",
        "**TranformerEncoder**\n",
        "\n",
        "Apply self-attention layers onto the source tokens.\n",
        "It only needs the source key padding mask.\n",
        "\n",
        "\n",
        "**TranformerDecoder**\n",
        "\n",
        "Apply masked self-attention layers to the target tokens and cross-attention\n",
        "layers between the source and the target tokens.\n",
        "It needs the source and target key padding masks, and the target attention mask."
      ],
      "metadata": {
        "id": "nIpHjOtK47DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"Single decoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO\n",
        "        self.attention = MultiheadAttention(\n",
        "            dim=d_model, \n",
        "            n_heads=nhead, \n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.multihead_attention = MultiheadAttention(\n",
        "            dim=d_model, \n",
        "            n_heads=nhead, \n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decode the next target tokens based on the previous tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "\n",
        "        # compute self-attention    \n",
        "        attn_out, _ = self.attention(tgt, tgt, tgt, \n",
        "                                attn_mask=tgt_mask_attn, key_padding_mask=tgt_key_padding_mask, \n",
        "                                ) \n",
        "        \n",
        "        attn_out = self.dropout1(attn_out)\n",
        "        attn_out = self.layer_norm1(tgt + attn_out)\n",
        "\n",
        "        masked_attn_out, _ = self.multihead_attention(attn_out, src, src,\n",
        "                                            attn_mask=None, key_padding_mask=src_key_padding_mask, \n",
        "                                            )\n",
        "        \n",
        "        # compute multi-head attention between source and target\n",
        "        masked_attn_out = self.dropout2(masked_attn_out)\n",
        "        masked_attn_out = self.layer_norm2(attn_out + masked_attn_out)\n",
        "\n",
        "        # compute feedforward layer\n",
        "        feedforward_out = self.dropout3(self.feed_forward(masked_attn_out))\n",
        "        y = self.layer_norm3(masked_attn_out + feedforward_out)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer decoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_decoder_layers: Number of stacked decoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            num_decoder_layer:int ,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_decoder_layer = num_decoder_layer\n",
        "\n",
        "        self.decoder_layer = TransformerDecoderLayer(\n",
        "            d_model=d_model, \n",
        "            d_ff=d_ff, \n",
        "            nhead=nhead,\n",
        "            dropout=dropout)\n",
        "        \n",
        "        self.decoder_layers = nn.ModuleList([self.decoder_layer for _ in range(num_decoder_layer)])\n",
        "\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decodes the source sequence by sequentially passing.\n",
        "        the encoded source sequence and the target sequence through the decoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of encoded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of taget sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        y = tgt\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            y = decoder_layer( \n",
        "                        tgt=y, \n",
        "                        tgt_mask_attn=tgt_mask_attn, \n",
        "                        src=src, \n",
        "                        src_key_padding_mask=src_key_padding_mask,\n",
        "                        tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        \n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Single encoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of input tokens.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = MultiheadAttention(\n",
        "            dim=d_model, \n",
        "            n_heads=nhead, \n",
        "            dropout=dropout)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.FloatTensor,\n",
        "        key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the input. Does not attend to masked inputs.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        attn_out, _ = self.attention(src , src , src , \n",
        "                                        attn_mask=None, \n",
        "                                        key_padding_mask=key_padding_mask \n",
        "                                        ) \n",
        "        attn_out = self.dropout1(attn_out)\n",
        "\n",
        "        attn_out = self.layer_norm1(src + attn_out)\n",
        "\n",
        "        feedforward_out = self.dropout2(self.feed_forward(attn_out))\n",
        "        y = self.layer_norm2(attn_out + feedforward_out) \n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer encoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders inputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            dim_feedforward: int,\n",
        "            num_encoder_layers: int,\n",
        "            nheads: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_decoder_layer = num_encoder_layers\n",
        "\n",
        "        self.encoder_layer = TransformerEncoderLayer(\n",
        "            d_model=d_model, \n",
        "            d_ff=dim_feedforward, \n",
        "            nhead = nheads,\n",
        "            dropout=dropout)\n",
        "        # Puts encoder layers in list \n",
        "        self.encoder_layers = nn.ModuleList([self.encoder_layer for _ in range(num_encoder_layers)])\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the source sequence by sequentially passing.\n",
        "        the source sequence through the encoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source sequence.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        y = src \n",
        "        for layer in self.encoder_layers:\n",
        "          # Calling the forward of each of each encoder layer\n",
        "            y = layer(y, key_padding_mask=key_padding_mask)\n",
        "        \n",
        "        return y"
      ],
      "metadata": {
        "id": "2d-ukpIOu_RH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main layers\n",
        "This section gather the `Transformer` and the `TranslationTransformer` modules.\n",
        "\n",
        "**Transformer**\n",
        "\n",
        "\n",
        "The classical transformer architecture.\n",
        "It takes the source and target tokens embeddings and\n",
        "do the forward pass through the encoder and decoder.\n",
        "\n",
        "**Translation Transformer**\n",
        "\n",
        "Compute the source and target tokens embeddings, and apply a final head to produce next token logits.\n",
        "The output must not be the softmax but just the logits, because we use the `nn.CrossEntropyLoss`.\n",
        "\n",
        "It also creates the *src_key_padding_mask*, the *tgt_key_padding_mask* and the *tgt_mask_attn*."
      ],
      "metadata": {
        "id": "Gd3kGoRO4_TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Implementation of a Transformer based on the paper: https://arxiv.org/pdf/1706.03762.pdf.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders/decoders inputs/ouputs.\n",
        "        nhead: Number of heads for each multi-head attention.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        num_decoder_layers: Number of stacked encoders.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            nhead: int,\n",
        "            num_encoder_layers: int,\n",
        "            num_decoder_layers: int,\n",
        "            dim_feedforward: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = TransformerEncoder(\n",
        "            d_model=d_model, \n",
        "            dim_feedforward=dim_feedforward,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            nheads=nhead,\n",
        "            dropout=dropout)\n",
        "        # Decoder \n",
        "        self.decoder = TransformerDecoder(\n",
        "            d_model=d_model, \n",
        "            d_ff=dim_feedforward,\n",
        "            num_decoder_layer=num_decoder_layers,\n",
        "            nhead=nhead,\n",
        "            dropout=dropout)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Compute next token embeddings.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sequences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sequences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Next token embeddings, given the previous target tokens and the source tokens.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(src=src, key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        decoder_out = self.decoder(\n",
        "                                tgt=tgt, \n",
        "                                src=encoder_out, \n",
        "                                tgt_mask_attn=tgt_mask_attn, \n",
        "                                src_key_padding_mask=src_key_padding_mask, \n",
        "                                tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        return decoder_out\n",
        "\n",
        "\n",
        "\n",
        "class TranslationTransformer(nn.Module):\n",
        "    \"\"\"Basic Transformer encoder and decoder for a translation task.\n",
        "    Manage the masks creation, and the token embeddings.\n",
        "    Position embeddings can be learnt with a standard `nn.Embedding` layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        n_heads: Number of heads for each multi-head attention.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the feedforward layers\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the encoder and decoder.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            n_heads: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = config[\"device\"]\n",
        "        self.max_seq_len = config[\"max_sequence_length\"]\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "\n",
        "        self.embedding_src = nn.Embedding(num_embeddings=n_tokens_src, embedding_dim=dim_embedding, padding_idx=src_pad_idx)\n",
        "        self.embedding_tgt = nn.Embedding(num_embeddings=n_tokens_tgt, embedding_dim=dim_embedding, padding_idx=tgt_pad_idx)\n",
        "        self.position_embedding = nn.Embedding(num_embeddings=self.max_seq_len, embedding_dim=dim_embedding)\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            d_model=dim_embedding,\n",
        "            nhead=n_heads, \n",
        "            num_encoder_layers=n_layers,\n",
        "            num_decoder_layers=n_layers,\n",
        "            dim_feedforward=dim_hidden, \n",
        "            dropout=dropout)\n",
        "        \n",
        "        # (N, T, dim_embedding) -> (N, T, n_tokens_tgt)\n",
        "        self.linear = nn.Linear(dim_embedding, n_tokens_tgt) # test\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim_embedding, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.Linear(128, n_tokens_tgt)\n",
        "        )\n",
        "\n",
        "\n",
        "    def make_key_padding_mask(self, x, pad_idx) -> torch.BoolTensor:\n",
        "        # Out : src_key_padding_mask (seq_len_src, seq_len_src)\n",
        "        return (x == pad_idx).to(self.device)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens logites based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, seq_len_tgt, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "        batch = source.size(0)\n",
        "        seq_len_src = source.size(1)\n",
        "        seq_len_tgt = target.size(1)\n",
        "        \n",
        "        src_positions = (torch.arange(0, seq_len_src).unsqueeze(0).expand(batch, seq_len_src)).to(self.device)\n",
        "        tgt_positions = (torch.arange(0, seq_len_tgt).unsqueeze(0).expand(batch, seq_len_tgt)).to(self.device)\n",
        "        src_emb = self.embedding_src(source) + self.position_embedding(src_positions) \n",
        "        tgt_emb = self.embedding_tgt(target) + self.position_embedding(tgt_positions) \n",
        "\n",
        "        square_subsequent_mask = torch.triu(torch.full((seq_len_tgt, seq_len_tgt), float('-inf')), diagonal=1)\n",
        "        tgt_mask = square_subsequent_mask.to(self.device) # (seq_len_tgt, seq_len_tgt)\n",
        "\n",
        "        src_key_padding_mask = self.make_key_padding_mask(source, self.src_pad_idx) # (batch, seq_len_src)\n",
        "        tgt_key_padding_mask = self.make_key_padding_mask(target, self.tgt_pad_idx) # (batch, seq_len_src)\n",
        "        \n",
        "\n",
        "\n",
        "        output = self.transformer(src=src_emb, tgt=tgt_emb, tgt_mask_attn=tgt_mask, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask) # (batch_size, seq_len_tgt, E (dim_embedding) )\n",
        "\n",
        "        # output = self.linear(output)\n",
        "        output = self.mlp(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "AGYVF34mvRNk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Greedy search\n",
        "\n",
        "Here you have to implement a geedy search to generate a target translation from a trained model and an input source string.\n",
        "The next token will simply be the most probable one."
      ],
      "metadata": {
        "id": "ql6jv2lAK-nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        max_sentence_length: int,\n",
        "    ) -> str:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces logits score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentence: The translated source sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    tgt_tokens = ['<bos>']\n",
        "    EOS_TOKEN = '<eos>'\n",
        "    \n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = src_vocab(src_tokens)\n",
        "\n",
        "    tgt_tokens = tgt_vocab(tgt_tokens)\n",
        "    \n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    model.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "      src_tokens = torch.LongTensor(src_tokens).unsqueeze(dim=0).to(device)\n",
        "      tgt_tokens = torch.LongTensor(tgt_tokens).unsqueeze(dim=0).to(device)\n",
        "      s = 0\n",
        "      predicted = model.forward(src_tokens, tgt_tokens)\n",
        "      \n",
        "      predicted = torch.softmax(predicted, dim=-1)\n",
        "      token_max = torch.argmax(predicted)\n",
        "\n",
        "      predicted_token = tgt_vocab.lookup_token(token_max)\n",
        "      predicted_sentence = [predicted_token]\n",
        "\n",
        "      while predicted_token is not EOS_TOKEN or len(predicted_sentence) < max_sentence_length:\n",
        "        predicted_token = torch.LongTensor(tgt_vocab(predicted_token)).unsqueeze(0).to(device)\n",
        "        predicted_token = model.forward(src_tokens, predicted_token)\n",
        "        predicted = torch.softmax(predicted, dim=-1)\n",
        "        token_max = torch.argmax(predicted)\n",
        "        predicted_token = tgt_vocab.lookup_token(token_max)\n",
        "\n",
        "        predicted_sentence.append(predicted_token)\n",
        "        s += 1"
      ],
      "metadata": {
        "id": "-KMp7piKK905"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam search\n",
        "Beam search is a smarter way of producing a sequence of tokens from\n",
        "an autoregressive model than just using a greedy search.\n",
        "\n",
        "The greedy search always choose the most probable token as the unique\n",
        "and only next target token, and repeat this processus until the *\\<eos\\>* token is predicted.\n",
        "\n",
        "Instead, the beam search selects the k-most probable tokens at each step.\n",
        "From those k tokens, the current sequence is duplicated k times and the k tokens are appended to the k sequences to produce new k sequences.\n",
        "\n",
        "*You don't have to understand this code, but understanding this code once the TP is over could improve your torch tensors skills.*\n",
        "\n",
        "---\n",
        "\n",
        "**More explanations**\n",
        "\n",
        "Since it is done at each step, the number of sequences grows exponentially (k sequences after the first step, k² sequences after the second...).\n",
        "In order to keep the number of sequences low, we remove sequences except the top-s most likely sequences.\n",
        "To do that, we keep track of the likelihood of each sequence.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "Then, we define the likelihood of a target sentence $t = [t_1, ..., t_{N_t}]$ as:\n",
        "\n",
        "$$\n",
        "L(t) = \\prod_{i=1}^{N_t - 1} p(t_{i+1} | s, t_{i}; \\theta )\n",
        "$$\n",
        "\n",
        "Pseudocode of the beam search:\n",
        "```\n",
        "source: [N_s source tokens]  # Shape of [total_source_tokens]\n",
        "target: [1, <bos> token]  # Shape of [n_sentences, current_target_tokens]\n",
        "target_prob: [1]  # Shape of [n_sentences]\n",
        "# We use `n_sentences` as the batch_size dimension\n",
        "\n",
        "while current_target_tokens <= max_target_length:\n",
        "    source = repeat(source, n_sentences)  # Shape of [n_sentences, total_source_tokens]\n",
        "    predicted = model(source, target)[:, -1]  # Predict the next token distributions of all the n_sentences\n",
        "    tokens_idx, tokens_prob = topk(predicted, k)\n",
        "\n",
        "    # Append the `n_sentences * k` tokens to the `n_sentences` sentences\n",
        "    target = repeat(target, k)  # Shape of [n_sentences * k, current_target_tokens]\n",
        "    target = append_tokens(target, tokens_idx)  # Shape of [n_sentences * k, current_target_tokens + 1]\n",
        "\n",
        "    # Update the sentences probabilities\n",
        "    target_prob = repeat(target_prob, k)  # Shape of [n_sentences * k]\n",
        "    target_prob *= tokens_prob\n",
        "\n",
        "    if n_sentences * k >= max_sentences:\n",
        "        target, target_prob = topk_prob(target, target_prob, k=max_sentences)\n",
        "    else:\n",
        "        n_sentences *= k\n",
        "\n",
        "    current_target_tokens += 1\n",
        "```"
      ],
      "metadata": {
        "id": "LgGFG-uXue6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beautify(sentence: str) -> str:\n",
        "    \"\"\"Removes useless spaces.\n",
        "    \"\"\"\n",
        "    punc = {'.', ',', ';'}\n",
        "    for p in punc:\n",
        "        sentence = sentence.replace(f' {p}', p)\n",
        "    \n",
        "    links = {'-', \"'\"}\n",
        "    for l in links:\n",
        "        sentence = sentence.replace(f'{l} ', l)\n",
        "        sentence = sentence.replace(f' {l}', l)\n",
        "    \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "V-GomgGTY2sV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V9Q7qcvH2Chp"
      },
      "outputs": [],
      "source": [
        "def indices_terminated(\n",
        "        target: torch.FloatTensor,\n",
        "        eos_token: int\n",
        "    ) -> tuple:\n",
        "    \"\"\"Split the target sentences between the terminated and the non-terminated\n",
        "    sentence. Return the indices of those two groups.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: The sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        eos_token: Value of the End-of-Sentence token.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        terminated: Indices of the terminated sentences (who's got the eos_token).\n",
        "            Shape of [n_terminated, ].\n",
        "        non-terminated: Indices of the unfinished sentences.\n",
        "            Shape of [batch_size-n_terminated, ].\n",
        "    \"\"\"\n",
        "    terminated = [i for i, t in enumerate(target) if eos_token in t]\n",
        "    non_terminated = [i for i, t in enumerate(target) if eos_token not in t]\n",
        "    return torch.LongTensor(terminated), torch.LongTensor(non_terminated)\n",
        "\n",
        "\n",
        "def append_beams(\n",
        "        target: torch.FloatTensor,\n",
        "        beams: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Add the beam tokens to the current sentences.\n",
        "    Duplicate the sentences so one token is added per beam per batch.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: Batch of unfinished sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        beams: Batch of beams for each sentences.\n",
        "            Shape of [batch_size, n_beams].\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        target: Batch of sentences with one beam per sentence.\n",
        "            Shape of [batch_size * n_beams, n_tokens+1].\n",
        "    \"\"\"\n",
        "    batch_size, n_beams = beams.shape\n",
        "    n_tokens = target.shape[1]\n",
        "\n",
        "    target = einops.repeat(target, 'b t -> b c t', c=n_beams)  \n",
        "    beams = beams.unsqueeze(dim=2)  \n",
        "\n",
        "    target = torch.cat((target, beams), dim=2)            # [batch_size, n_beams, n_tokens+1]\n",
        "    target = target.view(batch_size*n_beams, n_tokens+1)  # [batch_size * n_beams, n_tokens+1]\n",
        "    return target\n",
        "\n",
        "\n",
        "def beam_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        beam_width: int,\n",
        "        max_target: int,\n",
        "        max_sentence_length: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces linear score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        beam_width: Number of top-k tokens we keep at each stage.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentences: List of sentences orderer by their likelihood.\n",
        "    \"\"\"\n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = src_vocab(src_tokens)\n",
        "\n",
        "    tgt_tokens = ['<bos>']\n",
        "    tgt_tokens = tgt_vocab(tgt_tokens)\n",
        "\n",
        "    # To tensor and add unitary batch dimension\n",
        "    src_tokens = torch.LongTensor(src_tokens).to(device)\n",
        "    tgt_tokens = torch.LongTensor(tgt_tokens).unsqueeze(dim=0).to(device)\n",
        "    target_probs = torch.FloatTensor([1]).to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    with torch.no_grad():\n",
        "        while tgt_tokens.shape[1] < max_sentence_length:\n",
        "            batch_size, n_tokens = tgt_tokens.shape\n",
        "\n",
        "            # Get next beams\n",
        "            src = einops.repeat(src_tokens, 't -> b t', b=tgt_tokens.shape[0])\n",
        "            predicted = model.forward(src, tgt_tokens)\n",
        "            predicted = torch.softmax(predicted, dim=-1)\n",
        "            probs, predicted = predicted[:, -1].topk(k=beam_width, dim=-1)\n",
        "\n",
        "            # Separe between terminated sentences and the others\n",
        "            idx_terminated, idx_not_terminated = indices_terminated(tgt_tokens, EOS_IDX)\n",
        "            idx_terminated, idx_not_terminated = idx_terminated.to(device), idx_not_terminated.to(device)\n",
        "\n",
        "            tgt_terminated = torch.index_select(tgt_tokens, dim=0, index=idx_terminated)\n",
        "            tgt_probs_terminated = torch.index_select(target_probs, dim=0, index=idx_terminated)\n",
        "\n",
        "            filter_t = lambda t: torch.index_select(t, dim=0, index=idx_not_terminated)\n",
        "            tgt_others = filter_t(tgt_tokens)\n",
        "            tgt_probs_others = filter_t(target_probs)\n",
        "            predicted = filter_t(predicted)\n",
        "            probs = filter_t(probs)\n",
        "\n",
        "            # Add the top tokens to the previous target sentences\n",
        "            tgt_others = append_beams(tgt_others, predicted)\n",
        "\n",
        "            # Add padding to terminated target\n",
        "            padd = torch.zeros((len(tgt_terminated), 1), dtype=torch.long, device=device)\n",
        "            tgt_terminated = torch.cat(\n",
        "                (tgt_terminated, padd),\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            # Update each target sentence probabilities\n",
        "            tgt_probs_others = torch.repeat_interleave(tgt_probs_others, beam_width)\n",
        "            tgt_probs_others *= probs.flatten()\n",
        "            tgt_probs_terminated *= 0.999  # Penalize short sequences overtime\n",
        "\n",
        "            # Group up the terminated and the others\n",
        "            target_probs = torch.cat(\n",
        "                (tgt_probs_others, tgt_probs_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "            tgt_tokens = torch.cat(\n",
        "                (tgt_others, tgt_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "\n",
        "            # Keep only the top `max_target` target sentences\n",
        "            if target_probs.shape[0] <= max_target:\n",
        "                continue\n",
        "\n",
        "            target_probs, indices = target_probs.topk(k=max_target, dim=0)\n",
        "            tgt_tokens = torch.index_select(tgt_tokens, dim=0, index=indices)\n",
        "\n",
        "    sentences = []\n",
        "    for tgt_sentence in tgt_tokens:\n",
        "        tgt_sentence = list(tgt_sentence)[1:]  # Remove <bos> token\n",
        "        tgt_sentence = list(takewhile(lambda t: t != EOS_IDX, tgt_sentence))\n",
        "        tgt_sentence = ' '.join(tgt_vocab.lookup_tokens(tgt_sentence))\n",
        "        sentences.append(tgt_sentence)\n",
        "\n",
        "    sentences = [beautify(s) for s in sentences]\n",
        "\n",
        "    # Join the sentences with their likelihood\n",
        "    sentences = [(s, p.item()) for s, p in zip(sentences, target_probs)]\n",
        "    # Sort the sentences by their likelihood\n",
        "    sentences = [(s, p) for s, p in sorted(sentences, key=lambda k: k[1], reverse=True)]\n",
        "\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVr2FuDcZxC6"
      },
      "source": [
        "# Training loop\n",
        "This is a basic training loop code. It takes a big configuration dictionnary to avoid never ending arguments in the functions.\n",
        "We use [Weights and Biases](https://wandb.ai/) to log the trainings.\n",
        "It logs every training informations and model performances in the cloud.\n",
        "You have to create an account to use it. Every accounts are free for individuals or research teams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h2I1C8pRXN8j"
      },
      "outputs": [],
      "source": [
        "def print_logs(dataset_type: str, logs: dict):\n",
        "    \"\"\"Print the logs.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        dataset_type: Either \"Train\", \"Eval\", \"Test\" type.\n",
        "        logs: Containing the metric's name and value.\n",
        "    \"\"\"\n",
        "    desc = [\n",
        "        f'{name}: {value:.2f}'\n",
        "        for name, value in logs.items()\n",
        "    ]\n",
        "    desc = '\\t'.join(desc)\n",
        "    desc = f'{dataset_type} -\\t' + desc\n",
        "    desc = desc.expandtabs(5)\n",
        "    print(desc)\n",
        "\n",
        "\n",
        "def topk_accuracy(\n",
        "        real_tokens: torch.FloatTensor,\n",
        "        probs_tokens: torch.FloatTensor,\n",
        "        k: int,\n",
        "        tgt_pad_idx: int,\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Compute the top-k accuracy.\n",
        "    We ignore the PAD tokens.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        real_tokens: Real tokens of the target sentence.\n",
        "            Shape of [batch_size * n_tokens].\n",
        "        probs_tokens: Tokens probability predicted by the model.\n",
        "            Shape of [batch_size * n_tokens, n_target_vocabulary].\n",
        "        k: Top-k accuracy threshold.\n",
        "        src_pad_idx: Source padding index value.\n",
        "    \n",
        "    Output\n",
        "    ------\n",
        "        acc: Scalar top-k accuracy value.\n",
        "    \"\"\"\n",
        "    total = (real_tokens != tgt_pad_idx).sum()\n",
        "\n",
        "    _, pred_tokens = probs_tokens.topk(k=k, dim=-1)            # [batch_size * n_tokens, k]\n",
        "    real_tokens = einops.repeat(real_tokens, 'b -> b k', k=k)  # [batch_size * n_tokens, k]\n",
        "\n",
        "    good = (pred_tokens == real_tokens) & (real_tokens != tgt_pad_idx)\n",
        "    acc = good.sum() / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "def loss_batch(\n",
        "        model: nn.Module,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor,\n",
        "        config: dict,\n",
        "    )-> dict:\n",
        "    \"\"\"Compute the metrics associated with this batch.\n",
        "    The metrics are:\n",
        "        - loss\n",
        "        - top-1 accuracy\n",
        "        - top-5 accuracy\n",
        "        - top-10 accuracy\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The model to train.\n",
        "        source: Batch of source tokens.\n",
        "            Shape of [batch_size, n_src_tokens].\n",
        "        target: Batch of target tokens.\n",
        "            Shape of [batch_size, n_tgt_tokens].\n",
        "        config: Additional parameters.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        metrics: Dictionnary containing evaluated metrics on this batch.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    loss_fn = config['loss'].to(device)\n",
        "    metrics = dict()\n",
        "\n",
        "    source, target = source.to(device), target.to(device)\n",
        "    target_in, target_out = target[:, :-1], target[:, 1:]\n",
        "\n",
        "    # Loss\n",
        "    pred = model(source, target_in)      # [batch_size, n_tgt_tokens-1, n_vocab]\n",
        "    pred = pred.view(-1, pred.shape[2])  # [batch_size * (n_tgt_tokens - 1), n_vocab]\n",
        "    target_out = target_out.flatten()    # [batch_size * (n_tgt_tokens - 1),]\n",
        "    metrics['loss'] = loss_fn(pred, target_out)\n",
        "\n",
        "    # Accuracy - we ignore the padding predictions\n",
        "    for k in [1, 5, 10]:\n",
        "        metrics[f'top-{k}'] = topk_accuracy(target_out, pred, k, config['tgt_pad_idx'])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def eval_model(model: nn.Module, dataloader: DataLoader, config: dict) -> dict:\n",
        "    \"\"\"Evaluate the model on the given dataloader.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    logs = defaultdict(list)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for source, target in dataloader:\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())\n",
        "\n",
        "    for name, values in logs.items():\n",
        "        logs[name] = np.mean(values)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, config: dict):\n",
        "    \"\"\"Train the model in a teacher forcing manner.\n",
        "    \"\"\"\n",
        "    train_loader, val_loader = config['train_loader'], config['val_loader']\n",
        "    train_dataset, val_dataset = train_loader.dataset.dataset, val_loader.dataset.dataset\n",
        "    optimizer = config['optimizer']\n",
        "    clip = config['clip']\n",
        "    device = config['device']\n",
        "\n",
        "    columns = ['epoch']\n",
        "    for mode in ['train', 'validation']:\n",
        "        columns += [\n",
        "            f'{mode} - {colname}'\n",
        "            for colname in ['source', 'target', 'predicted', 'likelihood']\n",
        "        ]\n",
        "    log_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    print(f'Starting training for {config[\"epochs\"]} epochs, using {device}.')\n",
        "    for e in range(config['epochs']):\n",
        "        print(f'\\nEpoch {e+1}')\n",
        "\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "        logs = defaultdict(list)\n",
        "\n",
        "        for batch_id, (source, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            loss = metrics['loss']\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "        \n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())  # Don't forget the '.item' to free the cuda memory\n",
        "            \n",
        "            if batch_id % config['log_every'] == 0:\n",
        "                for name, value in logs.items():\n",
        "                    logs[name] = np.mean(value)\n",
        "\n",
        "                train_logs = {\n",
        "                    f'Train - {m}': v\n",
        "                    for m, v in logs.items()\n",
        "                }\n",
        "                wandb.log(train_logs)\n",
        "                logs = defaultdict(list)\n",
        "        \n",
        "        # Logs\n",
        "        if len(logs) != 0:\n",
        "            for name, value in logs.items():\n",
        "                logs[name] = np.mean(value)\n",
        "            train_logs = {\n",
        "                f'Train - {m}': v\n",
        "                for m, v in logs.items()\n",
        "            }\n",
        "        else:\n",
        "            logs = {\n",
        "                m.split(' - ')[1]: v\n",
        "                for m, v in train_logs.items()\n",
        "            }\n",
        "\n",
        "        print_logs('Train', logs)\n",
        "\n",
        "        logs = eval_model(model, val_loader, config)\n",
        "        print_logs('Eval', logs)\n",
        "        val_logs = {\n",
        "            f'Validation - {m}': v\n",
        "            for m, v in logs.items()\n",
        "        }\n",
        "\n",
        "        val_source, val_target = val_dataset[ torch.randint(len(val_dataset), (1,)) ]\n",
        "        val_pred, val_prob = beam_search(\n",
        "            model,\n",
        "            val_source,\n",
        "            config['src_vocab'],\n",
        "            config['tgt_vocab'],\n",
        "            config['src_tokenizer'],\n",
        "            device,  # It can take a lot of VRAM\n",
        "            beam_width=10,\n",
        "            max_target=100,\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "        print(val_source)\n",
        "        print(val_pred)\n",
        "\n",
        "        logs = {**train_logs, **val_logs}  # Merge dictionnaries\n",
        "        wandb.log(logs)  # Upload to the WandB cloud\n",
        "\n",
        "        # Table logs\n",
        "        train_source, train_target = train_dataset[ torch.randint(len(train_dataset), (1,)) ]\n",
        "        train_pred, train_prob = beam_search(\n",
        "            model,\n",
        "            train_source,\n",
        "            config['src_vocab'],\n",
        "            config['tgt_vocab'],\n",
        "            config['src_tokenizer'],\n",
        "            device,  # It can take a lot of VRAM\n",
        "            beam_width=10,\n",
        "            max_target=100,\n",
        "            max_sentence_length=config['max_sequence_length'],\n",
        "        )[0]\n",
        "\n",
        "        data = [\n",
        "            e + 1,\n",
        "            train_source, train_target, train_pred, train_prob,\n",
        "            val_source, val_target, val_pred, val_prob,\n",
        "        ]\n",
        "        log_table.add_data(*data)\n",
        "    \n",
        "    # Log the table at the end of the training\n",
        "    wandb.log({'Model predictions': log_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the models\n",
        "We can now finally train the models.\n",
        "Choose the right hyperparameters, play with them and try to find\n",
        "ones that lead to good models and good training curves.\n",
        "Try to reach a loss under 1.0.\n",
        "\n",
        "So you know, it is possible to get descent results with approximately 20 epochs.\n",
        "With CUDA enabled, one epoch, even on a big model with a big dataset, shouldn't last more than 10 minutes.\n",
        "A normal epoch is between 1 to 5 minutes.\n",
        "\n",
        "*This is considering Colab Pro, we should try using free Colab to get better estimations.*\n",
        "\n",
        "---\n",
        "\n",
        "To test your implementations, it is easier to try your models\n",
        "in a CPU instance. Indeed, Colab reduces your GPU instances priority\n",
        "with the time you recently past using GPU instances. It would be\n",
        "sad to consume all your GPU time on implementation testing.\n",
        "Moreover, you should try your models on small datasets and with a small number of parameters.\n",
        "For exemple, you could set:\n",
        "```\n",
        "MAX_SEQ_LEN = 10\n",
        "MIN_TOK_FREQ = 20\n",
        "dim_embedding = 40\n",
        "dim_hidden = 60\n",
        "n_layers = 1\n",
        "```\n",
        "\n",
        "You usually don't want to log anything onto WandB when testing your implementation.\n",
        "To deactivate WandB without having to change any line of code, you can type `!wandb offline` in a cell.\n",
        "\n",
        "Once you have rightly implemented the models, you can train bigger models on bigger datasets.\n",
        "When you do this, do not forget to change the runtime as GPU (and use `!wandb online`)!"
      ],
      "metadata": {
        "id": "YImgxCWjlWni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking GPU and logging to wandb\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wandb login\n",
        "# 4635e7b3acf7ef8066e44259dbb7c61cd48a3dda\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "WriScTUEsRHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a7accc-e096-493d-f481-ca6f625adcf2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayzeg\u001b[0m (\u001b[33m8225_team_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Mon Apr 10 09:08:13 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P0    50W / 400W |   1281MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciate the datasets\n",
        "# MAX_SEQ_LEN = 10\n",
        "# MIN_TOK_FREQ = 20\n",
        "\n",
        "MAX_SEQ_LEN = 60\n",
        "MIN_TOK_FREQ = 2\n",
        "train_dataset, val_dataset = build_datasets(\n",
        "    MAX_SEQ_LEN,\n",
        "    MIN_TOK_FREQ,\n",
        "    en_tokenizer,\n",
        "    fr_tokenizer,\n",
        "    train,\n",
        "    valid,\n",
        ")\n",
        "\n",
        "print(f'English vocabulary size: {len(train_dataset.en_vocab):,}')\n",
        "print(f'French vocabulary size: {len(train_dataset.fr_vocab):,}')\n",
        "\n",
        "print(f'\\nTraining examples: {len(train_dataset):,}')\n",
        "print(f'Validation examples: {len(val_dataset):,}')"
      ],
      "metadata": {
        "id": "iqmpxnO1lgDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57665a82-6426-4ec8-e1c6-681089674d8f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 11,753\n",
            "French vocabulary size: 17,820\n",
            "\n",
            "Training examples: 196,176\n",
            "Validation examples: 21,797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ywFEpplOU5dn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05b14f2-da42-479c-de4f-2f0f2319ab9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return super().__sizeof__() + self.nbytes()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "Layer (type:depth-idx)                             Output Shape              Param #\n",
              "====================================================================================================\n",
              "TranslationTransformer                             [128, 60, 17820]          730,620\n",
              "├─Embedding: 1-1                                   [128, 60, 40]             470,120\n",
              "├─Embedding: 1-2                                   [128, 60, 40]             2,400\n",
              "├─Embedding: 1-3                                   [128, 60, 40]             712,800\n",
              "├─Embedding: 1-4                                   [128, 60, 40]             (recursive)\n",
              "├─Transformer: 1-5                                 [128, 60, 40]             --\n",
              "│    └─TransformerEncoder: 2-1                     [128, 60, 40]             --\n",
              "│    │    └─ModuleList: 3-1                        --                        11,620\n",
              "│    └─TransformerDecoder: 2-2                     [128, 60, 40]             --\n",
              "│    │    └─ModuleList: 3-2                        --                        18,260\n",
              "├─Sequential: 1-6                                  [128, 60, 17820]          --\n",
              "│    └─Linear: 2-3                                 [128, 60, 128]            5,248\n",
              "│    └─LeakyReLU: 2-4                              [128, 60, 128]            --\n",
              "│    └─LayerNorm: 2-5                              [128, 60, 128]            256\n",
              "│    └─Linear: 2-6                                 [128, 60, 128]            16,512\n",
              "│    └─LeakyReLU: 2-7                              [128, 60, 128]            --\n",
              "│    └─LayerNorm: 2-8                              [128, 60, 128]            256\n",
              "│    └─Linear: 2-9                                 [128, 60, 128]            16,512\n",
              "│    └─LeakyReLU: 2-10                             [128, 60, 128]            --\n",
              "│    └─LayerNorm: 2-11                             [128, 60, 128]            256\n",
              "│    └─Linear: 2-12                                [128, 60, 17820]          2,298,780\n",
              "====================================================================================================\n",
              "Total params: 4,283,640\n",
              "Trainable params: 4,283,640\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 455.09\n",
              "====================================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 1205.94\n",
              "Params size (MB): 14.21\n",
              "Estimated Total Size (MB): 1220.28\n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "config = {\n",
        "    # General parameters\n",
        "    'epochs': 12,\n",
        "    'batch_size': 128,\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196,\n",
        "    'dim_hidden': 256,\n",
        "    'n_layers': 3,\n",
        "    'dim_embedding': 40,\n",
        "    'dim_hidden': 60,\n",
        "    'n_layers': 1,\n",
        "    'dropout': 0.1,\n",
        "    'model_type': 'GRU',\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "\n",
        "config['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config['src_pad_idx'], config['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "# model = TranslationRNN(\n",
        "#     config['n_tokens_src'],\n",
        "#     config['n_tokens_tgt'],\n",
        "#     config['dim_embedding'],\n",
        "#     config['dim_hidden'],\n",
        "#     config['n_layers'],\n",
        "#     config['dropout'],\n",
        "#     config['src_pad_idx'],\n",
        "#     config['tgt_pad_idx'],\n",
        "#     config['model_type'],\n",
        "# )\n",
        "\n",
        "model = TranslationTransformer(\n",
        "    config['n_tokens_src'],\n",
        "    config['n_tokens_tgt'],\n",
        "    config['n_heads'],\n",
        "    config['dim_embedding'],\n",
        "    config['dim_hidden'],\n",
        "    config['n_layers'],\n",
        "    config['dropout'],\n",
        "    config['src_pad_idx'],\n",
        "    config['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "\n",
        "config['optimizer'] = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    betas=config['betas'],\n",
        ")\n",
        "\n",
        "weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "config['loss'] = nn.CrossEntropyLoss(\n",
        "    weight=weight_classes,\n",
        "    ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        ")\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=[\n",
        "        (config['batch_size'], config['max_sequence_length']),\n",
        "        (config['batch_size'], config['max_sequence_length'])\n",
        "    ],\n",
        "    dtypes=[torch.long, torch.long],\n",
        "    depth=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wandb online\n",
        "!wandb login --relogin \n",
        "!WANDB_MODE=online"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li5jtjYJxvtk",
        "outputId": "4dc51227-b651-4742-b671-2197dacbd918"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "maOTVtk4acxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4133244e-dff4-44b5-8d44-416ef93f3873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        }
      ],
      "source": [
        "!wandb online  # online / offline to activate or deactivate WandB logging\n",
        "\n",
        "# print(\"Config \", config)\n",
        "\n",
        "def train(config=config): # Needed for sweeps only\n",
        "  with wandb.init(\n",
        "          config=config,\n",
        "          project='INF8225 - TP3',  # Title of your project\n",
        "          group='TranslationTransformer run group',  # In what group of runs do you want this run to be in?\n",
        "          save_code=True,\n",
        "      ) as run:\n",
        "      # config = wandb.config\n",
        "      # run.name = \n",
        "      train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9PFIyvKUefdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7adde63-8a5c-49e8-8ce4-82f94aee56fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. (5.81071%) \t C'est possible d'essayer ton travail ici.\n",
            "1. (4.09919%) \t C'est possible de tenter votre travail ici.\n",
            "2. (2.32683%) \t Il est possible d'essayer ton travail ici.\n",
            "3. (2.06878%) \t C'est possible de tenter ton travail ici.\n",
            "4. (1.95494%) \t Il est possible de tenter votre travail ici.\n"
          ]
        }
      ],
      "source": [
        "sentence = \"It is possible to try your work here.\"\n",
        "\n",
        "preds = beam_search(\n",
        "    model,\n",
        "    sentence,\n",
        "    config['src_vocab'],\n",
        "    config['tgt_vocab'],\n",
        "    config['src_tokenizer'],\n",
        "    config['device'],\n",
        "    beam_width=10,\n",
        "    max_target=100,\n",
        "    max_sentence_length=config['max_sequence_length']\n",
        ")[:5]\n",
        "\n",
        "for i, (translation, likelihood) in enumerate(preds):\n",
        "    print(f'{i}. ({likelihood*100:.5f}%) \\t {translation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "1. Explain the differences between Vanilla RNN, GRU-RNN, and Transformers. \n",
        "\n",
        "Vanilla RNN implements a basic cell design, applying a tanh function between the hidden and source inputs. In contrast, a GRU cell incorporates additional connections, known as gates, including a reset and an update gate. These gates possess their own weights and enable the passage of extra information, which enhances the GRU cell's ability to preserve long-term information.\n",
        "\n",
        "GRU-RNN employs two gates to tackle the vanishing gradient challenge:\n",
        "\n",
        "    An update gate : which assesses how much the prior hidden state should be influenced by the current input vector (i.e., the impact of the subsequent word on the preceding layer's outcome).\n",
        "    A reset gate : which determines the amount of the previous hidden state that should be disregarded.\n",
        "\n",
        "RNNs exhibit faster training and require less computational power compared to GRU-RNNs. Nevertheless, GRU-RNNs surpass RNNs when handling extensive sequences.\n",
        "\n",
        "With GRU-RNN, words at a sentence's conclusion exert a more substantial effect on upcoming predictions, a problem stemming from recursion.\n",
        "\n",
        "On the other hand, the transformer architecture deviates from RNN/GRU-RNN due to the absence of recurrent connections. All tokens within a sentence are processed simultaneously, eliminating the notion of timesteps. An attention mechanism is utilized to maintain information about the sequence's history.\n",
        "\n",
        "2. Why is positionnal encoding necessary in Transformers and not in RNNs?\n",
        "\n",
        "RNNs feature recurrent connections, allowing the current block to receive the last input as supplementary input, thus providing an understanding of word placement within the sequence input. Conversely, in Transformers, each input within a sequence is independent, rendering Transformers incapable of distinguishing between sequences with jumbled words. To address this, positional encoding is employed.\n",
        "\n",
        "3. Describe the preprocessing process. Detail how the initial dataset is processed before being fed to the translation models.\n",
        "\n",
        "The process begins with a text file containing two columns, one with the English source sequence and the other with the corresponding French translation, each line representing a distinct sentence pair. Sentences are then separated into lists of tokens ( For example, 'Nice to meet you !' becomes ['Nice','to','meet','you','!'] ).\n",
        "\n",
        "Next, the Vocab() class is employed to convert these token lists into one-hot encodings with additional Special tokens (\\<bos>) and (\\<eos>) as delimiters for each sequence. ( For example ['\\<bos>', 'my', 'pleasure', '\\<eos>'] -> [0, 4, 15, 1])\n",
        "\n",
        "When a PyTorch DataLoader is created from the tokenized dataset, a function is triggered to append a special padding token (\\<pad>) to ensure that all token batches are of the same size.\n",
        "A special padding (\\<pad>) is finally appended to all token when a PyTorch DataLoader is created from the tokenized data, to ensure token batches are the same size."
      ],
      "metadata": {
        "id": "uHhixEEGzWRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Small report - experiments\n",
        "Once everything is working fine, you can explore aspects of these models and do some research of your own into how they behave.\n",
        "\n",
        "For exemple, you can experiment with the hyperparameters.\n",
        "What are the effect of the differents hyperparameters with the final model performance? What about training time?\n",
        "\n",
        "What are some other metrics you could have for machine translation? Can you compute them and add them to your WandB report?\n",
        "\n",
        "Those are only examples, you can do whatever you think will be interesting.\n",
        "This part account for many points, *feel free to go wild!*\n",
        "\n",
        "---\n",
        "*Make a concise report about your experiments here.*"
      ],
      "metadata": {
        "id": "Y3tQdusIjPCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters search (sweep)"
      ],
      "metadata": {
        "id": "WlrgGGKzCn8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 sweeps have been defined (RNN, GRU , transformer)"
      ],
      "metadata": {
        "id": "r0MhaplWIOD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the RNN and GRU, good results seem to come from the models with 6~7 epochs, so we will fix 6 epochs for our following hyperparameter search. For the transformer, the number of epochs will be 12."
      ],
      "metadata": {
        "id": "fPARXJjZ4aXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(\"INF8225 - TP3/ \", train, count=10) # RNN"
      ],
      "metadata": {
        "id": "ZzQDrp2PTjxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f67f483a-2c32-47a7-a4e5-d9653b4ec59b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ooz8sege with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 370\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.27825276330947457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009833031838853794\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayzeg\u001b[0m (\u001b[33m8225_team_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_062824-ooz8sege</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ooz8sege' target=\"_blank\">vocal-sweep-4</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ooz8sege' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ooz8sege</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 3.37     top-1: 0.38    top-5: 0.58    top-10: 0.66\n",
            "Eval -    loss: 3.35     top-1: 0.38    top-5: 0.57    top-10: 0.65\n",
            "I'm beginning to lose patience with Tom.\n",
            "Je ne sais rien.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 3.19     top-1: 0.41    top-5: 0.60    top-10: 0.67\n",
            "Eval -    loss: 3.15     top-1: 0.41    top-5: 0.60    top-10: 0.68\n",
            "We want to fix that problem.\n",
            "Nous sommes prêtes.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 3.07     top-1: 0.42    top-5: 0.61    top-10: 0.69\n",
            "Eval -    loss: 3.07     top-1: 0.41    top-5: 0.61    top-10: 0.69\n",
            "Nothing lasts forever.\n",
            "Quand j'étais enfant.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 3.01     top-1: 0.43    top-5: 0.62    top-10: 0.70\n",
            "Eval -    loss: 3.03     top-1: 0.42    top-5: 0.62    top-10: 0.69\n",
            "He was playing the piano.\n",
            "Il y a beaucoup d'amis.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.91     top-1: 0.43    top-5: 0.63    top-10: 0.71\n",
            "Eval -    loss: 2.95     top-1: 0.43    top-5: 0.63    top-10: 0.70\n",
            "Get ready.\n",
            "« ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.90     top-1: 0.44    top-5: 0.64    top-10: 0.71\n",
            "Eval -    loss: 2.94     top-1: 0.44    top-5: 0.63    top-10: 0.71\n",
            "Too many sweets make you fat.\n",
            "Selon moi !\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███████████████</td></tr><tr><td>Train - top-10</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▃▂▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▅██</td></tr><tr><td>Validation - top-10</td><td>▁▄▆▆██</td></tr><tr><td>Validation - top-5</td><td>▁▄▆▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.89893</td></tr><tr><td>Train - top-1</td><td>0.43852</td></tr><tr><td>Train - top-10</td><td>0.71247</td></tr><tr><td>Train - top-5</td><td>0.63624</td></tr><tr><td>Validation - loss</td><td>2.94027</td></tr><tr><td>Validation - top-1</td><td>0.43506</td></tr><tr><td>Validation - top-10</td><td>0.70537</td></tr><tr><td>Validation - top-5</td><td>0.63057</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vocal-sweep-4</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ooz8sege' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ooz8sege</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_062824-ooz8sege/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xz282xbu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 73\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.22992617160098855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006994117711121344\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_063423-xz282xbu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xz282xbu' target=\"_blank\">rare-sweep-5</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xz282xbu' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xz282xbu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.83     top-1: 0.45    top-5: 0.65    top-10: 0.72\n",
            "Eval -    loss: 2.90     top-1: 0.44    top-5: 0.64    top-10: 0.71\n",
            "Tom made a list of things he wanted to do before he died.\n",
            "Tom n'a pas le choix.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.81     top-1: 0.45    top-5: 0.65    top-10: 0.72\n",
            "Eval -    loss: 2.87     top-1: 0.44    top-5: 0.64    top-10: 0.72\n",
            "I'm going back to my office.\n",
            "Je suis désolé ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.73     top-1: 0.45    top-5: 0.66    top-10: 0.74\n",
            "Eval -    loss: 2.85     top-1: 0.45    top-5: 0.64    top-10: 0.72\n",
            "I assure you Tom will be perfectly safe.\n",
            "Je suis désolé.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.74     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "Eval -    loss: 2.84     top-1: 0.45    top-5: 0.65    top-10: 0.72\n",
            "Please keep me informed.\n",
            "Quand j'étais enfant !\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.71     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "Eval -    loss: 2.83     top-1: 0.45    top-5: 0.65    top-10: 0.72\n",
            "She is too young to know the truth.\n",
            "Elle me rend nerveux.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.69     top-1: 0.46    top-5: 0.66    top-10: 0.74\n",
            "Eval -    loss: 2.83     top-1: 0.45    top-5: 0.65    top-10: 0.72\n",
            "Who's that woman over there?\n",
            "Ça te dérange ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▆▆▇█▇██▆▆▆▇▇▇▇▅▆▅▆▆▆▂▄▄▅▆▅▅▂▃▄▄▅▄▁▃▃▄▃▄▃</td></tr><tr><td>Train - top-1</td><td>▃▂▃▁▃▂▂▂▃▃▂▃▃▃▄▂▅▄▄▅▆▅▆▅▄▅▆▇▆▇▅▆▇█▆▇▆▇▆█</td></tr><tr><td>Train - top-10</td><td>▂▁▁▁▂▁▁▃▃▂▁▂▂▁▃▃▄▄▃▄▇▅▄▅▄▄▄▆▆▅▅▄▆█▆▆▅▇▆▆</td></tr><tr><td>Train - top-5</td><td>▂▁▁▁▂▁▁▃▃▂▁▂▂▂▃▃▄▄▃▄█▅▅▆▄▄▄▇▆▆▅▅▇█▆▆▅▇▇▇</td></tr><tr><td>Validation - loss</td><td>█▅▃▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▇█▇</td></tr><tr><td>Validation - top-10</td><td>▁▃▅▇▇█</td></tr><tr><td>Validation - top-5</td><td>▁▃▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.6862</td></tr><tr><td>Train - top-1</td><td>0.46161</td></tr><tr><td>Train - top-10</td><td>0.73852</td></tr><tr><td>Train - top-5</td><td>0.66399</td></tr><tr><td>Validation - loss</td><td>2.83212</td></tr><tr><td>Validation - top-1</td><td>0.45059</td></tr><tr><td>Validation - top-10</td><td>0.72233</td></tr><tr><td>Validation - top-5</td><td>0.64873</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rare-sweep-5</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xz282xbu' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xz282xbu</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_063423-xz282xbu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8bpyw80j with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 700\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 289\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.13130109068684753\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009352356160543272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_064023-8bpyw80j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/8bpyw80j' target=\"_blank\">effortless-sweep-6</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/8bpyw80j' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/8bpyw80j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.67     top-1: 0.46    top-5: 0.67    top-10: 0.74\n",
            "Eval -    loss: 2.82     top-1: 0.45    top-5: 0.65    top-10: 0.72\n",
            "I let the cat in.\n",
            "J'ai fait ça.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.65     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Eval -    loss: 2.81     top-1: 0.46    top-5: 0.65    top-10: 0.73\n",
            "I would like you to trust me.\n",
            "Je l'aime.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.63     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Eval -    loss: 2.79     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "Hone your skills.\n",
            "Le temps est écoulé.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.62     top-1: 0.47    top-5: 0.67    top-10: 0.75\n",
            "Eval -    loss: 2.79     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "What this club is today is largely due to the effort of these people.\n",
            "Ce n'est pas vrai.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.63     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Eval -    loss: 2.79     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "Give me a call later, OK?\n",
            "Je peux nager ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.61     top-1: 0.47    top-5: 0.67    top-10: 0.75\n",
            "Eval -    loss: 2.78     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "Clothes don't make the man.\n",
            "Les choses changent.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▄▆▆▇▇██▄▆▇▇▇▇▇▄▄▅▆▇▇▁▅▄▅▅▆▆▁▄▄▅▅▆▁▂▄▄▅▄▅</td></tr><tr><td>Train - top-1</td><td>▃▂▃▂▄▂▁▃▃▁▁▄▃▃▃▅▄▃▃▃▆▄▅▅▆▂▅▇▅▅▅▅▄██▅▅▅█▆</td></tr><tr><td>Train - top-10</td><td>▄▃▂▁▂▁▁▄▃▂▁▂▁▁▄▄▃▃▂▃█▃▄▄▄▂▃▇▄▄▄▄▄█▆▅▅▄▆▄</td></tr><tr><td>Train - top-5</td><td>▄▃▂▁▃▁▁▄▃▁▂▃▂▂▄▅▄▃▂▃█▄▅▄▅▃▄▇▄▆▄▅▄█▆▅▆▄▆▅</td></tr><tr><td>Validation - loss</td><td>█▅▂▃▂▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▆▆▇█</td></tr><tr><td>Validation - top-10</td><td>▁▃▆▆▇█</td></tr><tr><td>Validation - top-5</td><td>▁▃▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.61123</td></tr><tr><td>Train - top-1</td><td>0.47132</td></tr><tr><td>Train - top-10</td><td>0.74525</td></tr><tr><td>Train - top-5</td><td>0.67314</td></tr><tr><td>Validation - loss</td><td>2.783</td></tr><tr><td>Validation - top-1</td><td>0.46099</td></tr><tr><td>Validation - top-10</td><td>0.72984</td></tr><tr><td>Validation - top-5</td><td>0.65733</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">effortless-sweep-6</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/8bpyw80j' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/8bpyw80j</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_064023-8bpyw80j/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tyz03tut with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 651\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.07011145463813385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006600273023362729\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_064623-tyz03tut</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/tyz03tut' target=\"_blank\">leafy-sweep-7</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/tyz03tut' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/tyz03tut</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.58     top-1: 0.48    top-5: 0.68    top-10: 0.75\n",
            "Eval -    loss: 2.77     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "Who's your favorite super hero?\n",
            "Que dirais-tu ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.58     top-1: 0.48    top-5: 0.68    top-10: 0.75\n",
            "Eval -    loss: 2.77     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "While he was sick, he lost a lot of weight.\n",
            "Même s'il vous plaît.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.60     top-1: 0.47    top-5: 0.68    top-10: 0.75\n",
            "Eval -    loss: 2.77     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "I have eaten a lot this morning.\n",
            "J'ai fait ça.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.54     top-1: 0.48    top-5: 0.68    top-10: 0.76\n",
            "Eval -    loss: 2.76     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "I didn't know you were expecting anyone.\n",
            "Je ne peux pas faire ça.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.52     top-1: 0.48    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.76     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "It was a revelation to me.\n",
            "Ce fut une excuse bidon.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.51     top-1: 0.49    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.77     top-1: 0.46    top-5: 0.66    top-10: 0.73\n",
            "I want to go and see the cherry trees in blossom.\n",
            "Je veux t'aider.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▄▅▇▆▆▇▇▄▅▆▆▆██▄▅▄▆▇█▂▄▅▆▅▆▆▃▄▄▄▆▆▁▃▄▄▅▄▆</td></tr><tr><td>Train - top-1</td><td>▃▃▂▃▃▃▃▃▃▃▂▄▂▂▄▃▅▃▂▁▆▅▄▃▅▄▄▄▆▅▆▃▄█▄▅▅▄▇▅</td></tr><tr><td>Train - top-10</td><td>▅▃▂▃▃▃▃▅▃▃▃▃▃▁▅▃▅▄▁▂▇▄▆▄▄▃▄▆▆▆▅▃▄█▇▆▆▅▆▄</td></tr><tr><td>Train - top-5</td><td>▄▂▁▃▃▃▃▃▃▃▂▃▂▁▅▃▅▃▂▁▆▃▅▃▄▂▄▆▆▅▆▃▄█▆▅▅▄▆▄</td></tr><tr><td>Validation - loss</td><td>▄█▆▂▁█</td></tr><tr><td>Validation - top-1</td><td>▇▁▄██▅</td></tr><tr><td>Validation - top-10</td><td>▃▁▂██▆</td></tr><tr><td>Validation - top-5</td><td>▁▁▃█▇▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.51018</td></tr><tr><td>Train - top-1</td><td>0.48612</td></tr><tr><td>Train - top-10</td><td>0.7624</td></tr><tr><td>Train - top-5</td><td>0.68778</td></tr><tr><td>Validation - loss</td><td>2.77283</td></tr><tr><td>Validation - top-1</td><td>0.46288</td></tr><tr><td>Validation - top-10</td><td>0.73263</td></tr><tr><td>Validation - top-5</td><td>0.66092</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">leafy-sweep-7</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/tyz03tut' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/tyz03tut</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_064623-tyz03tut/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zepyrp0f with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 577\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 575\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.253616715822834\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.000770141803485162\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_065222-zepyrp0f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zepyrp0f' target=\"_blank\">twilight-sweep-8</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zepyrp0f' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zepyrp0f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.53     top-1: 0.48    top-5: 0.68    top-10: 0.76\n",
            "Eval -    loss: 2.75     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Tom died at a very old age.\n",
            "Elle lui a conseillé d'arrêter de fumer.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.53     top-1: 0.48    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.75     top-1: 0.47    top-5: 0.66    top-10: 0.74\n",
            "I've finished all except the last page.\n",
            "J'ai fait une erreur.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.55     top-1: 0.48    top-5: 0.68    top-10: 0.76\n",
            "Eval -    loss: 2.75     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I'm too short to reach the top shelf.\n",
            "Je suis né.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.51     top-1: 0.49    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "You sing as well as you dance.\n",
            "Tu parles.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.50     top-1: 0.49    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.75     top-1: 0.47    top-5: 0.66    top-10: 0.74\n",
            "It was horrendous.\n",
            "Ce fut une erreur.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.48     top-1: 0.49    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.75     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Of course!\n",
            "Du calme.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▄▅▅▆▇▇▂▅▅▅▆▇▇▃▄▅▅▇█▁▃▄▅▅▆▆▁▄▄▅▅▆▁▂▄▄▅▅▆</td></tr><tr><td>Train - top-1</td><td>▄▅▃▄▃▃▃▆▃▃▅▄▃▃▄▅▄▄▂▁▇▅▇▄▄▄▅█▆▅▄▄▆▇▆▅▅▄▅▄</td></tr><tr><td>Train - top-10</td><td>▄▄▃▃▂▂▁▇▃▃▃▃▁▁▄▄▄▄▂▁▇▅▅▄▄▄▃▇▄▄▄▄▃█▆▅▄▄▄▃</td></tr><tr><td>Train - top-5</td><td>▄▄▃▄▃▂▂▅▃▃▄▃▁▂▅▄▄▄▂▁▇▅▄▃▄▃▃▇▅▄▄▃▄█▅▄▄▄▅▃</td></tr><tr><td>Validation - loss</td><td>▅█▆▁▆▂</td></tr><tr><td>Validation - top-1</td><td>▂▁▃█▃█</td></tr><tr><td>Validation - top-10</td><td>▅▁▃█▅█</td></tr><tr><td>Validation - top-5</td><td>▆▄▆▇▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.47857</td></tr><tr><td>Train - top-1</td><td>0.48715</td></tr><tr><td>Train - top-10</td><td>0.76385</td></tr><tr><td>Train - top-5</td><td>0.69236</td></tr><tr><td>Validation - loss</td><td>2.74602</td></tr><tr><td>Validation - top-1</td><td>0.47018</td></tr><tr><td>Validation - top-10</td><td>0.73715</td></tr><tr><td>Validation - top-5</td><td>0.66607</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">twilight-sweep-8</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zepyrp0f' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zepyrp0f</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_065222-zepyrp0f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xaj0a64f with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 422\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.19391275354122212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.000645164751201379\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_065827-xaj0a64f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xaj0a64f' target=\"_blank\">treasured-sweep-9</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xaj0a64f' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xaj0a64f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.49     top-1: 0.49    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I'm skinny.\n",
            "Je suis désolée.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.46     top-1: 0.49    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "That's a lame excuse.\n",
            "Il nous faut partir.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.49     top-1: 0.49    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I know now what we have to do.\n",
            "Je sais que Tom est parti.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.48     top-1: 0.49    top-5: 0.69    top-10: 0.77\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Life without books is unimaginable.\n",
            "Une minute.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.49     top-1: 0.49    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I told you the truth.\n",
            "Je m'appelle Tom.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.44     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Who knows?\n",
            "Tu rentres ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▃▄▆▆▇█▃▄▅▆▇▆▆▃▅▅▇▆▆▂▃▃▅▅▅█▃▃▄▅▇▆▁▄▄▄▆▆▆</td></tr><tr><td>Train - top-1</td><td>▅▅▄▂▃▃▁▄▄▄▂▃▅▄▅▃▄▃▄▄▆▇▇▅▅▆▂▆▅▄▅▁▄█▅▇▆▃▄▃</td></tr><tr><td>Train - top-10</td><td>▆▅▄▂▃▃▁▆▅▃▃▁▄▃▆▄▄▂▃▃█▆▅▄▄▄▃▆▆▄▄▂▃█▆▅▅▃▃▄</td></tr><tr><td>Train - top-5</td><td>▅▆▄▃▃▃▁▆▅▃▂▂▄▄▆▄▅▃▃▄▇▆▆▄▄▅▃▆▆▄▄▁▄█▅▇▆▃▄▄</td></tr><tr><td>Validation - loss</td><td>█▆▅▁▅▄</td></tr><tr><td>Validation - top-1</td><td>▃█▆▁▅▄</td></tr><tr><td>Validation - top-10</td><td>▁▂▅█▇▄</td></tr><tr><td>Validation - top-5</td><td>▃▂▁█▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.4434</td></tr><tr><td>Train - top-1</td><td>0.49629</td></tr><tr><td>Train - top-10</td><td>0.76742</td></tr><tr><td>Train - top-5</td><td>0.69725</td></tr><tr><td>Validation - loss</td><td>2.7391</td></tr><tr><td>Validation - top-1</td><td>0.47063</td></tr><tr><td>Validation - top-10</td><td>0.73844</td></tr><tr><td>Validation - top-5</td><td>0.66792</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">treasured-sweep-9</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xaj0a64f' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xaj0a64f</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_065827-xaj0a64f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s5ilk9hp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 402\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 144\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2394009804916092\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009383851854113872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_070431-s5ilk9hp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/s5ilk9hp' target=\"_blank\">ethereal-sweep-10</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/s5ilk9hp' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/s5ilk9hp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.47     top-1: 0.49    top-5: 0.69    top-10: 0.76\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I rewrote it.\n",
            "J'ai fait ça.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.43     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Tom lived in Boston for a long time.\n",
            "Tom a l'air surpris.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.43     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.73     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I can hear you, but I can't see you.\n",
            "Je suis désolé.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.43     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I kind of liked them.\n",
            "Je jouais au football.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.41     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.73     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "If it's possible, I want to go home now.\n",
            "Si tu te trompes.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.43     top-1: 0.49    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Tickets are $30, parking is free and children under ten receive free admission.\n",
            "Une tasse.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▂▄▆▅▇██▃▄▅▇▆█▇▃▅▅▇▇█▁▅▅▆▇▆▇▂▃▆▆▆▇▁▂▄▅▅▆█</td></tr><tr><td>Train - top-1</td><td>█▅▄▅▂▁▂▆▃▅▂▄▂▃▅▃▅▃▃▂█▃▅▅▃▅▃▇▅▂▃▄▄▇▆▄▅▆▄▃</td></tr><tr><td>Train - top-10</td><td>█▅▃▂▂▁▂▆▅▄▃▂▂▂▅▄▄▂▂▂█▄▄▂▂▄▃▇▅▂▄▂▂█▇▅▄▄▃▁</td></tr><tr><td>Train - top-5</td><td>▇▅▃▂▂▁▂▇▅▄▂▃▂▃▅▃▄▂▃▂█▅▄▃▂▄▃▇▆▂▄▃▃█▇▅▅▄▄▂</td></tr><tr><td>Validation - loss</td><td>▇█▁█▄▆</td></tr><tr><td>Validation - top-1</td><td>▁█▇▃▆▇</td></tr><tr><td>Validation - top-10</td><td>▃▂█▁▇▆</td></tr><tr><td>Validation - top-5</td><td>▁▃█▂██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.4344</td></tr><tr><td>Train - top-1</td><td>0.49492</td></tr><tr><td>Train - top-10</td><td>0.77142</td></tr><tr><td>Train - top-5</td><td>0.69922</td></tr><tr><td>Validation - loss</td><td>2.73602</td></tr><tr><td>Validation - top-1</td><td>0.47396</td></tr><tr><td>Validation - top-10</td><td>0.7407</td></tr><tr><td>Validation - top-5</td><td>0.67019</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ethereal-sweep-10</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/s5ilk9hp' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/s5ilk9hp</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_070431-s5ilk9hp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ue80qxhc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 451\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2263208499636016\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006378170606560637\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_071036-ue80qxhc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ue80qxhc' target=\"_blank\">good-sweep-11</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ue80qxhc' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ue80qxhc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.44     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I don't want to talk about that right now.\n",
            "Je ne veux pas faire ça.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.44     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.73     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I know that Japanese songs are very difficult for us.\n",
            "Je sais tout.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.39     top-1: 0.50    top-5: 0.71    top-10: 0.78\n",
            "Eval -    loss: 2.73     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "Do you have a pet?\n",
            "Vous êtes-vous ?\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.39     top-1: 0.50    top-5: 0.71    top-10: 0.77\n",
            "Eval -    loss: 2.73     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "We don't need you anymore.\n",
            "Vos cheveux sont beaux.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.41     top-1: 0.50    top-5: 0.70    top-10: 0.78\n",
            "Eval -    loss: 2.73     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "I ran away from home when I was thirteen years old.\n",
            "J'étudie le français.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.40     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.73     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "I made him go.\n",
            "Je vous verrai demain.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▂▃▅▆▇▇▇▃▄▄▅▇▇█▃▄▅▆▇▇▁▄▅▆▆▇▆▃▄▄▅▆▆▁▃▃▆▆▆▆</td></tr><tr><td>Train - top-1</td><td>▆▅▅▃▁▁▃▅▅▇▄▃▁▁▄▇▃▅▂▂▇▅▄▅▄▃▅▅▄▅▆▆▄█▇▇▂▅▄▆</td></tr><tr><td>Train - top-10</td><td>▆▄▃▂▁▁▁▅▄▄▃▂▁▂▄▅▂▃▁▂▇▄▃▂▂▂▃▅▄▅▃▂▃█▅▄▂▂▃▃</td></tr><tr><td>Train - top-5</td><td>▇▄▃▂▁▁▁▅▅▄▄▂▁▁▄▆▂▃▁▂█▅▃▃▃▃▄▅▄▅▄▃▃█▅▆▂▂▃▃</td></tr><tr><td>Validation - loss</td><td>█▁▃▂▄▂</td></tr><tr><td>Validation - top-1</td><td>▁▄▅█▄▇</td></tr><tr><td>Validation - top-10</td><td>▁▆▁█▇▅</td></tr><tr><td>Validation - top-5</td><td>▁▇▃▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.40289</td></tr><tr><td>Train - top-1</td><td>0.50154</td></tr><tr><td>Train - top-10</td><td>0.77374</td></tr><tr><td>Train - top-5</td><td>0.70213</td></tr><tr><td>Validation - loss</td><td>2.72835</td></tr><tr><td>Validation - top-1</td><td>0.47632</td></tr><tr><td>Validation - top-10</td><td>0.74119</td></tr><tr><td>Validation - top-5</td><td>0.6715</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">good-sweep-11</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ue80qxhc' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ue80qxhc</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_071036-ue80qxhc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u9u2okzh with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 762\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1626407766577888\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008301226893111945\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_071636-u9u2okzh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/u9u2okzh' target=\"_blank\">scarlet-sweep-12</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/u9u2okzh' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/u9u2okzh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.40     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.73     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "You're not allowed to park here.\n",
            "Tu ne sais pas.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.39     top-1: 0.50    top-5: 0.71    top-10: 0.78\n",
            "Eval -    loss: 2.72     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "The prime minister fell into the Danube and drowned.\n",
            "Le soleil brillait.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.39     top-1: 0.50    top-5: 0.70    top-10: 0.78\n",
            "Eval -    loss: 2.72     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "I think Tom didn't understand what you were saying.\n",
            "Je pense que c'est vrai.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.40     top-1: 0.50    top-5: 0.70    top-10: 0.78\n",
            "Eval -    loss: 2.72     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "I thought we could get together later.\n",
            "Je suis désolé.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.39     top-1: 0.50    top-5: 0.71    top-10: 0.78\n",
            "Eval -    loss: 2.73     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "Farewell, my friend!\n",
            "Au Japon !\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.40     top-1: 0.50    top-5: 0.70    top-10: 0.78\n",
            "Eval -    loss: 2.73     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "You're still growing.\n",
            "Tu parles.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▁▄▄▅▆▇█▂▃▅▆▆▆▆▃▄▄▅▆▇▁▄▄▆▆▆▇▁▃▅▅▇▇▁▄▄▄▆▆▆</td></tr><tr><td>Train - top-1</td><td>█▅▄▆▄▄▁▆▅▄▅▄▆▄▇▇▇▅▅▄▇▆▇▅▅▄▂▇▆▆▅▅▃█▄▄▆▅▅▄</td></tr><tr><td>Train - top-10</td><td>█▄▄▅▃▃▁▆▆▃▃▂▃▃▅▅▆▃▃▂█▅▄▃▃▃▃▇▆▄▄▃▁█▅▄▄▃▄▃</td></tr><tr><td>Train - top-5</td><td>▇▃▄▅▃▃▁▆▆▃▃▃▄▃▅▅▇▃▄▂▇▅▅▃▃▄▂▇▆▅▄▃▂█▅▄▄▃▅▃</td></tr><tr><td>Validation - loss</td><td>▅▂▁▂▆█</td></tr><tr><td>Validation - top-1</td><td>▆▆█▆▁▃</td></tr><tr><td>Validation - top-10</td><td>▄▆▅▇█▁</td></tr><tr><td>Validation - top-5</td><td>▁▃█▆▂▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.39823</td></tr><tr><td>Train - top-1</td><td>0.50202</td></tr><tr><td>Train - top-10</td><td>0.77532</td></tr><tr><td>Train - top-5</td><td>0.70386</td></tr><tr><td>Validation - loss</td><td>2.73463</td></tr><tr><td>Validation - top-1</td><td>0.47504</td></tr><tr><td>Validation - top-10</td><td>0.7424</td></tr><tr><td>Validation - top-5</td><td>0.67323</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">scarlet-sweep-12</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/u9u2okzh' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/u9u2okzh</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_071636-u9u2okzh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zx6xuvpk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 360\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.17781868034215215\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0007024670687252072\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_072236-zx6xuvpk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zx6xuvpk' target=\"_blank\">robust-sweep-1</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/1t5pcxwf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zx6xuvpk' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zx6xuvpk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.39     top-1: 0.50    top-5: 0.70    top-10: 0.78\n",
            "Eval -    loss: 2.72     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "He put in a good word for me.\n",
            "Il se mit à pleuvoir.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.40     top-1: 0.50    top-5: 0.70    top-10: 0.77\n",
            "Eval -    loss: 2.74     top-1: 0.47    top-5: 0.67    top-10: 0.74\n",
            "My hands are stained with paint.\n",
            "Ma mère est morte.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.38     top-1: 0.50    top-5: 0.71    top-10: 0.78\n",
            "Eval -    loss: 2.72     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "She graduated with honors.\n",
            "Tom demanda à Mary.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.39     top-1: 0.50    top-5: 0.71    top-10: 0.78\n",
            "Eval -    loss: 2.73     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "Everything was ready for the trip.\n",
            "Merci.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.38     top-1: 0.50    top-5: 0.71    top-10: 0.77\n",
            "Eval -    loss: 2.72     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "Who were you hoping to talk to?\n",
            "Tu voulais savoir ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.38     top-1: 0.51    top-5: 0.71    top-10: 0.78\n",
            "Eval -    loss: 2.72     top-1: 0.48    top-5: 0.67    top-10: 0.74\n",
            "I have to take the chance.\n",
            "J'étudie le français.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▃▅▅▆▇▇▃▄▅▅█▇█▄▅▅▇▆▇▁▅▄▆▆██▃▄▅▆▆▇▁▃▅▅▆▇▇</td></tr><tr><td>Train - top-1</td><td>▆▇▄▄▄▂▄▄▄▃▄▁▃▁▃▅▅▂▃▃█▄▆▃▄▁▃▅▄▅▄▄▂█▅▃▄▄▃▂</td></tr><tr><td>Train - top-10</td><td>▆▅▅▄▃▃▃▆▅▄▄▂▃▁▄▄▅▂▃▃█▅▅▃▃▂▃▆▅▄▄▅▃█▆▅▄▄▃▂</td></tr><tr><td>Train - top-5</td><td>▆▅▄▄▃▂▃▅▅▄▃▁▂▁▄▃▄▁▃▂█▄▅▂▂▁▃▆▄▄▄▄▃█▅▄▄▄▁▂</td></tr><tr><td>Validation - loss</td><td>▁█▃▆▂▂</td></tr><tr><td>Validation - top-1</td><td>▇▁▇▅▆█</td></tr><tr><td>Validation - top-10</td><td>▃▁▅▃▅█</td></tr><tr><td>Validation - top-5</td><td>▄▁▅▄▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.38196</td></tr><tr><td>Train - top-1</td><td>0.50647</td></tr><tr><td>Train - top-10</td><td>0.77522</td></tr><tr><td>Train - top-5</td><td>0.70705</td></tr><tr><td>Validation - loss</td><td>2.72131</td></tr><tr><td>Validation - top-1</td><td>0.47908</td></tr><tr><td>Validation - top-10</td><td>0.74434</td></tr><tr><td>Validation - top-5</td><td>0.67483</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">robust-sweep-1</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zx6xuvpk' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/zx6xuvpk</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_072236-zx6xuvpk/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(\"INF8225 - TP3/kob3h22b\", train, count=10) # GRU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uaWhFlRJvstr",
        "outputId": "2781c8fd-cb56-49d7-ebf3-d1c243fc9ea7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: touyswa2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 360\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.17781868034215215\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0007024670687252072\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_072838-touyswa2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/touyswa2' target=\"_blank\">efficient-sweep-1</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/touyswa2' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/touyswa2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 3.09     top-1: 0.44    top-5: 0.62    top-10: 0.69\n",
            "Eval -    loss: 3.05     top-1: 0.44    top-5: 0.62    top-10: 0.69\n",
            "She asked him to give her some money.\n",
            "Elle m'a demandé à la maison.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.79     top-1: 0.48    top-5: 0.66    top-10: 0.72\n",
            "Eval -    loss: 2.76     top-1: 0.47    top-5: 0.66    top-10: 0.72\n",
            "Whether we succeed or not, we have to do our best.\n",
            "Les gens ne m'ont pas dit à Tom de faire ça.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.59     top-1: 0.50    top-5: 0.69    top-10: 0.75\n",
            "Eval -    loss: 2.61     top-1: 0.49    top-5: 0.68    top-10: 0.74\n",
            "I could've done better, I think.\n",
            "Je lui ai demandé à Tom.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.48     top-1: 0.51    top-5: 0.70    top-10: 0.76\n",
            "Eval -    loss: 2.51     top-1: 0.51    top-5: 0.70    top-10: 0.76\n",
            "Neither of my brothers will be there.\n",
            "Mes parents sont à Tom.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.36     top-1: 0.53    top-5: 0.72    top-10: 0.78\n",
            "Eval -    loss: 2.43     top-1: 0.52    top-5: 0.71    top-10: 0.77\n",
            "She began to sing.\n",
            "Elle est malade.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.29     top-1: 0.54    top-5: 0.73    top-10: 0.79\n",
            "Eval -    loss: 2.37     top-1: 0.53    top-5: 0.72    top-10: 0.78\n",
            "How many letters are there in the English alphabet?\n",
            "Combien de temps es-tu à Boston ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>Train - top-10</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Train - top-5</td><td>▁▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▂▂▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇█</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇█</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.28725</td></tr><tr><td>Train - top-1</td><td>0.53545</td></tr><tr><td>Train - top-10</td><td>0.7887</td></tr><tr><td>Train - top-5</td><td>0.73003</td></tr><tr><td>Validation - loss</td><td>2.36853</td></tr><tr><td>Validation - top-1</td><td>0.52694</td></tr><tr><td>Validation - top-10</td><td>0.77958</td></tr><tr><td>Validation - top-5</td><td>0.71928</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">efficient-sweep-1</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/touyswa2' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/touyswa2</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_072838-touyswa2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lfbtmukp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 731\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.12045946109992596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008779021020786119\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_073712-lfbtmukp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/lfbtmukp' target=\"_blank\">valiant-sweep-2</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/lfbtmukp' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/lfbtmukp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.23     top-1: 0.55    top-5: 0.74    top-10: 0.80\n",
            "Eval -    loss: 2.32     top-1: 0.53    top-5: 0.73    top-10: 0.79\n",
            "Were you listening to the radio yesterday?\n",
            "T'es-tu allé à la maison ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.17     top-1: 0.55    top-5: 0.75    top-10: 0.81\n",
            "Eval -    loss: 2.26     top-1: 0.54    top-5: 0.74    top-10: 0.80\n",
            "Please turn over.\n",
            "Prenez place, s'il vous plaît.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.16     top-1: 0.55    top-5: 0.75    top-10: 0.81\n",
            "Eval -    loss: 2.23     top-1: 0.55    top-5: 0.74    top-10: 0.80\n",
            "I'd like you to make one now.\n",
            "J'aimerais que tu sois là.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.09     top-1: 0.56    top-5: 0.76    top-10: 0.82\n",
            "Eval -    loss: 2.19     top-1: 0.55    top-5: 0.75    top-10: 0.81\n",
            "My right leg hurts.\n",
            "Ma lampe de la maison.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.04     top-1: 0.57    top-5: 0.77    top-10: 0.83\n",
            "Eval -    loss: 2.16     top-1: 0.56    top-5: 0.75    top-10: 0.81\n",
            "You shouldn't have paid the bill.\n",
            "Tu n'aurais pas dû téléphoner.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.97     top-1: 0.58    top-5: 0.78    top-10: 0.84\n",
            "Eval -    loss: 2.14     top-1: 0.56    top-5: 0.76    top-10: 0.81\n",
            "Birch trees have white bark.\n",
            "Des arbres portaient de la colline.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▆▇▇▇▇▇█▅▆▆▆▆▇▆▄▅▅▅▅▅▃▄▄▄▄▄▅▂▃▃▃▄▄▁▂▂▃▂▃▃</td></tr><tr><td>Train - top-1</td><td>▂▂▂▂▂▂▁▃▃▃▂▃▃▃▄▄▄▄▅▅▆▅▅▅▅▅▅▇▆▆▆▆▆█▇▇▆▇▇▇</td></tr><tr><td>Train - top-10</td><td>▂▂▂▂▂▂▁▃▃▃▃▃▃▃▅▄▄▄▅▅▆▆▅▅▅▅▅▇▆▆▆▆▆█▇▇▇▇▇▆</td></tr><tr><td>Train - top-5</td><td>▂▁▂▂▂▂▁▃▃▃▃▃▃▃▄▄▄▄▅▅▆▅▅▅▅▅▅▇▆▆▆▆▆█▇▇▇▇▇▆</td></tr><tr><td>Validation - loss</td><td>█▆▄▃▂▁</td></tr><tr><td>Validation - top-1</td><td>▁▃▄▆▇█</td></tr><tr><td>Validation - top-10</td><td>▁▃▅▆▇█</td></tr><tr><td>Validation - top-5</td><td>▁▃▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.97076</td></tr><tr><td>Train - top-1</td><td>0.57857</td></tr><tr><td>Train - top-10</td><td>0.83682</td></tr><tr><td>Train - top-5</td><td>0.78123</td></tr><tr><td>Validation - loss</td><td>2.13888</td></tr><tr><td>Validation - top-1</td><td>0.56052</td></tr><tr><td>Validation - top-10</td><td>0.81467</td></tr><tr><td>Validation - top-5</td><td>0.75758</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">valiant-sweep-2</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/lfbtmukp' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/lfbtmukp</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_073712-lfbtmukp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cbb2ybco with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 908\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.12753689232983317\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008649158741300643\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_074547-cbb2ybco</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/cbb2ybco' target=\"_blank\">stoic-sweep-3</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/cbb2ybco' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/cbb2ybco</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.95     top-1: 0.58    top-5: 0.78    top-10: 0.84\n",
            "Eval -    loss: 2.11     top-1: 0.56    top-5: 0.76    top-10: 0.82\n",
            "Tom lives and works in Boston.\n",
            "Tom habite jusqu'à Boston.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.90     top-1: 0.59    top-5: 0.79    top-10: 0.84\n",
            "Eval -    loss: 2.10     top-1: 0.57    top-5: 0.76    top-10: 0.82\n",
            "We thought that you were married.\n",
            "Nous pensions que tu étais surpris.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.88     top-1: 0.59    top-5: 0.79    top-10: 0.85\n",
            "Eval -    loss: 2.08     top-1: 0.57    top-5: 0.77    top-10: 0.82\n",
            "I hope you're well paid.\n",
            "J'espère que vous avez faim.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.82     top-1: 0.59    top-5: 0.80    top-10: 0.85\n",
            "Eval -    loss: 2.07     top-1: 0.57    top-5: 0.77    top-10: 0.83\n",
            "I rewrote it.\n",
            "Je suis resté silencieux.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.83     top-1: 0.59    top-5: 0.80    top-10: 0.85\n",
            "Eval -    loss: 2.04     top-1: 0.58    top-5: 0.78    top-10: 0.83\n",
            "We all have kids.\n",
            "Nous avons toutes nos enfants.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.81     top-1: 0.60    top-5: 0.80    top-10: 0.86\n",
            "Eval -    loss: 2.03     top-1: 0.58    top-5: 0.78    top-10: 0.83\n",
            "Let's not act rashly.\n",
            "Ne bouge pas.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▆▇▇▇███▅▆▆▇▇█▇▅▆▅▅▆▆▂▄▄▅▅▅▅▂▃▄▄▅▆▁▂▃▄▃▅▄</td></tr><tr><td>Train - top-1</td><td>▂▂▂▂▁▁▂▃▃▃▂▃▁▂▄▃▄▅▃▄▆▅▅▅▄▄▄▇▇▆▅▅▄██▇▆▇▅▆</td></tr><tr><td>Train - top-10</td><td>▃▁▂▂▁▁▁▃▃▂▂▂▂▂▄▃▄▄▃▃▆▅▄▄▄▄▄▆▆▅▅▄▃█▇▆▅▆▅▅</td></tr><tr><td>Train - top-5</td><td>▃▁▂▃▁▁▁▃▃▃▂▂▂▃▄▃▄▄▃▄▆▅▅▅▄▄▅▆▆▅▅▅▄█▇▆▆▇▅▆</td></tr><tr><td>Validation - loss</td><td>█▇▅▄▂▁</td></tr><tr><td>Validation - top-1</td><td>▁▃▄▅▇█</td></tr><tr><td>Validation - top-10</td><td>▁▂▄▄▇█</td></tr><tr><td>Validation - top-5</td><td>▁▂▄▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.811</td></tr><tr><td>Train - top-1</td><td>0.59774</td></tr><tr><td>Train - top-10</td><td>0.85682</td></tr><tr><td>Train - top-5</td><td>0.80187</td></tr><tr><td>Validation - loss</td><td>2.03014</td></tr><tr><td>Validation - top-1</td><td>0.57855</td></tr><tr><td>Validation - top-10</td><td>0.83191</td></tr><tr><td>Validation - top-5</td><td>0.77738</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stoic-sweep-3</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/cbb2ybco' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/cbb2ybco</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_074547-cbb2ybco/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pkqkwsiu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 370\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.27825276330947457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009833031838853794\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_075422-pkqkwsiu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/pkqkwsiu' target=\"_blank\">ancient-sweep-4</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/pkqkwsiu' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/pkqkwsiu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.78     top-1: 0.60    top-5: 0.80    top-10: 0.86\n",
            "Eval -    loss: 2.02     top-1: 0.58    top-5: 0.78    top-10: 0.83\n",
            "We'll be there in less than three hours.\n",
            "Nous nous reverrons trois jours.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.74     top-1: 0.61    top-5: 0.81    top-10: 0.87\n",
            "Eval -    loss: 2.01     top-1: 0.58    top-5: 0.78    top-10: 0.84\n",
            "Tom's boss advanced him a week's wages.\n",
            "Le chat de Tom a eu une attaque cardiaque.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.77     top-1: 0.60    top-5: 0.81    top-10: 0.86\n",
            "Eval -    loss: 2.00     top-1: 0.58    top-5: 0.78    top-10: 0.84\n",
            "I can't find my watch.\n",
            "Je ne peux pas vendre mon appartement.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.70     top-1: 0.62    top-5: 0.82    top-10: 0.87\n",
            "Eval -    loss: 1.98     top-1: 0.59    top-5: 0.79    top-10: 0.84\n",
            "It's incredibly easy to cheat the system.\n",
            "C'est intéressant.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.70     top-1: 0.62    top-5: 0.82    top-10: 0.87\n",
            "Eval -    loss: 1.98     top-1: 0.59    top-5: 0.79    top-10: 0.84\n",
            "We don't care what he does.\n",
            "Nous ne prêtes pas quoi faire.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.67     top-1: 0.62    top-5: 0.82    top-10: 0.87\n",
            "Eval -    loss: 1.98     top-1: 0.59    top-5: 0.79    top-10: 0.84\n",
            "I slept a little during lunch break because I was so tired.\n",
            "J'ai dormi trop froid hier soir.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▄▅▆▇▇██▄▅▆▆▆▇▇▄▄▅▇▆▇▂▄▅▅▅▆▆▂▃▅▅▅▅▁▁▃▄▄▅▅</td></tr><tr><td>Train - top-1</td><td>▄▃▃▃▂▁▁▄▄▃▃▃▃▃▅▅▃▃▃▃▆▅▅▅▄▄▄▇▆▅▅▄▄█▇▆▆▅▅▅</td></tr><tr><td>Train - top-10</td><td>▅▄▃▂▂▁▁▅▄▃▃▃▂▃▅▅▄▂▃▂▆▅▄▄▄▃▃▆▆▄▄▅▄█▇▅▅▅▄▄</td></tr><tr><td>Train - top-5</td><td>▄▃▃▂▂▁▁▄▄▃▃▃▂▂▄▄▃▂▃▂▆▄▄▄▄▃▄▆▆▄▅▄▄█▇▆▅▅▄▅</td></tr><tr><td>Validation - loss</td><td>█▇▄▂▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▂▄▇██</td></tr><tr><td>Validation - top-10</td><td>▁▃▄▆▇█</td></tr><tr><td>Validation - top-5</td><td>▁▃▄▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.66716</td></tr><tr><td>Train - top-1</td><td>0.62014</td></tr><tr><td>Train - top-10</td><td>0.87373</td></tr><tr><td>Train - top-5</td><td>0.82337</td></tr><tr><td>Validation - loss</td><td>1.97705</td></tr><tr><td>Validation - top-1</td><td>0.58895</td></tr><tr><td>Validation - top-10</td><td>0.84041</td></tr><tr><td>Validation - top-5</td><td>0.78763</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ancient-sweep-4</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/pkqkwsiu' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/pkqkwsiu</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_075422-pkqkwsiu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bynjied2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 73\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.22992617160098855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006994117711121344\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_080306-bynjied2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/bynjied2' target=\"_blank\">deft-sweep-5</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/bynjied2' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/bynjied2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.64     top-1: 0.63    top-5: 0.83    top-10: 0.88\n",
            "Eval -    loss: 1.96     top-1: 0.59    top-5: 0.79    top-10: 0.84\n",
            "I think everything is functional.\n",
            "Je pense que tout va bien.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.66     top-1: 0.62    top-5: 0.82    top-10: 0.87\n",
            "Eval -    loss: 1.96     top-1: 0.59    top-5: 0.79    top-10: 0.84\n",
            "Go now.\n",
            "mieux maintenant.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.63     top-1: 0.63    top-5: 0.83    top-10: 0.88\n",
            "Eval -    loss: 1.96     top-1: 0.59    top-5: 0.79    top-10: 0.84\n",
            "He became a policeman.\n",
            "Il devint professeur.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.62     top-1: 0.63    top-5: 0.83    top-10: 0.88\n",
            "Eval -    loss: 1.95     top-1: 0.59    top-5: 0.79    top-10: 0.84\n",
            "I'll be late for school!\n",
            "Je serai de retour tôt. »\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.58     top-1: 0.63    top-5: 0.83    top-10: 0.88\n",
            "Eval -    loss: 1.94     top-1: 0.60    top-5: 0.79    top-10: 0.85\n",
            "He was among those chosen.\n",
            "Il a été complètement parvenu.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.59     top-1: 0.63    top-5: 0.83    top-10: 0.88\n",
            "Eval -    loss: 1.93     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "I don't know what I've been so afraid of.\n",
            "Je ne sais pas ce que Tom sera heureux.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▄▅▅▇███▄▅▅▆▆██▄▅▅▇▆▇▂▄▅▅▆▆▇▁▃▄▅▆▆▂▂▄▄▅▆▆</td></tr><tr><td>Train - top-1</td><td>▅▄▄▂▁▁▁▅▄▄▄▃▁▁▅▄▃▃▃▃█▅▄▄▃▃▃█▆▅▄▅▃█▆▅▆▅▃▄</td></tr><tr><td>Train - top-10</td><td>▅▄▄▂▁▁▂▅▄▄▃▃▂▁▅▄▄▂▃▂▇▅▄▄▃▃▃█▆▅▄▃▃▇▆▅▅▄▃▃</td></tr><tr><td>Train - top-5</td><td>▅▄▄▂▁▁▁▅▄▄▃▃▂▁▄▄▃▂▃▃▇▅▄▄▃▃▃█▆▅▄▃▃▇▆▅▅▄▃▄</td></tr><tr><td>Validation - loss</td><td>█▇▇▄▃▁</td></tr><tr><td>Validation - top-1</td><td>▁▂▄▅▆█</td></tr><tr><td>Validation - top-10</td><td>▁▃▄▄▆█</td></tr><tr><td>Validation - top-5</td><td>▁▂▄▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.58603</td></tr><tr><td>Train - top-1</td><td>0.63178</td></tr><tr><td>Train - top-10</td><td>0.88436</td></tr><tr><td>Train - top-5</td><td>0.83487</td></tr><tr><td>Validation - loss</td><td>1.935</td></tr><tr><td>Validation - top-1</td><td>0.59726</td></tr><tr><td>Validation - top-10</td><td>0.84754</td></tr><tr><td>Validation - top-5</td><td>0.79509</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deft-sweep-5</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/bynjied2' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/bynjied2</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_080306-bynjied2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n78tw4wd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 700\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 289\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.13130109068684753\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009352356160543272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_081149-n78tw4wd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/n78tw4wd' target=\"_blank\">good-sweep-6</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/n78tw4wd' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/n78tw4wd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.55     top-1: 0.64    top-5: 0.84    top-10: 0.89\n",
            "Eval -    loss: 1.94     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "I have to get up anyways.\n",
            "Il me faut être arrêté hier.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.59     top-1: 0.63    top-5: 0.84    top-10: 0.89\n",
            "Eval -    loss: 1.92     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "Do you need this book?\n",
            "Avez-vous besoin de ce livre ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.54     top-1: 0.64    top-5: 0.84    top-10: 0.89\n",
            "Eval -    loss: 1.93     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "I live in a small fishing village.\n",
            "Je vis dans une petite île.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.56     top-1: 0.64    top-5: 0.84    top-10: 0.89\n",
            "Eval -    loss: 1.92     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "I'd appreciate your help.\n",
            "J'ai apprécié votre aide.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.55     top-1: 0.64    top-5: 0.84    top-10: 0.89\n",
            "Eval -    loss: 1.92     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "While traveling in Europe, I was pickpocketed on a train.\n",
            "En temps, je me frappe à sept heures.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.52     top-1: 0.64    top-5: 0.84    top-10: 0.89\n",
            "Eval -    loss: 1.91     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "I know what to expect.\n",
            "Je sais ce que nous pouvons.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▄▅▇▇▇█▃▄▅▇▆▆█▄▄▄▆▆▇▁▄▄▆▅▆▇▃▄▄▆▅▆▂▃▄▅▅▅▆</td></tr><tr><td>Train - top-1</td><td>▆▄▃▂▂▃▁▆▅▄▁▃▃▂▅▅▄▄▃▃█▅▅▂▄▃▂▆▅▅▄▄▄█▆▆▅▅▅▃</td></tr><tr><td>Train - top-10</td><td>▆▅▄▂▂▂▁▆▅▄▃▄▃▂▆▄▅▄▃▂█▅▅▄▄▃▂▆▆▅▃▄▃▇▆▅▄▄▅▃</td></tr><tr><td>Train - top-5</td><td>▆▅▄▂▂▂▁▆▅▄▃▃▃▂▆▄▅▄▃▃█▅▆▄▄▃▂▆▅▆▄▅▄▇▆▆▄▅▅▃</td></tr><tr><td>Validation - loss</td><td>█▄▆▂▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▂▁▇██</td></tr><tr><td>Validation - top-10</td><td>▁▃▂▅▅█</td></tr><tr><td>Validation - top-5</td><td>▁▃▂▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.51933</td></tr><tr><td>Train - top-1</td><td>0.64206</td></tr><tr><td>Train - top-10</td><td>0.89156</td></tr><tr><td>Train - top-5</td><td>0.84346</td></tr><tr><td>Validation - loss</td><td>1.91465</td></tr><tr><td>Validation - top-1</td><td>0.60205</td></tr><tr><td>Validation - top-10</td><td>0.85042</td></tr><tr><td>Validation - top-5</td><td>0.79959</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">good-sweep-6</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/n78tw4wd' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/n78tw4wd</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_081149-n78tw4wd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jcgivjoq with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 651\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.07011145463813385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006600273023362729\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_082033-jcgivjoq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/jcgivjoq' target=\"_blank\">dutiful-sweep-7</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/jcgivjoq' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/jcgivjoq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.51     top-1: 0.65    top-5: 0.84    top-10: 0.89\n",
            "Eval -    loss: 1.91     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "He lied about his age.\n",
            "Il a menti à son âge.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.49     top-1: 0.65    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.91     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "I can't understand this word.\n",
            "Je ne comprends pas ce mot.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.50     top-1: 0.65    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.91     top-1: 0.60    top-5: 0.80    top-10: 0.85\n",
            "Thanks to all of you.\n",
            "Merci pour tout.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.50     top-1: 0.65    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.90     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "Last summer, I had a chance to go to Boston, but I didn't go.\n",
            "La nuit dernière, j'ai pu me rendre visite à Tom demain.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.44     top-1: 0.66    top-5: 0.85    top-10: 0.90\n",
            "Eval -    loss: 1.90     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "Tom and Mary decorated their house for Christmas.\n",
            "Tom et Mary ont construit la veille de Noël.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.49     top-1: 0.65    top-5: 0.85    top-10: 0.90\n",
            "Eval -    loss: 1.90     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "Get to the point.\n",
            "Sors de moi.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▅▄▆▇█▇▃▃▅▆▆▇▇▃▄▆▆▆█▁▄▄▅▆▇▇▁▄▃▅▆▆▁▂▄▄▅▅▆</td></tr><tr><td>Train - top-1</td><td>▅▄▄▂▂▁▃▆▅▄▂▂▂▃▅▄▂▃▃▂█▅▄▃▃▃▂▇▅▅▃▃▄█▆▅▅▅▄▃</td></tr><tr><td>Train - top-10</td><td>▆▄▆▃▂▁▂▇▅▄▃▃▃▂▆▅▃▃▃▂█▅▅▃▃▃▂█▅▅▄▃▂█▇▅▅▅▄▃</td></tr><tr><td>Train - top-5</td><td>▆▄▅▂▂▁▁▆▆▃▃▃▃▃▅▅▃▂▃▂█▅▅▃▃▃▂█▅▅▄▃▂█▇▅▅▅▄▃</td></tr><tr><td>Validation - loss</td><td>▇█▅▁▅▃</td></tr><tr><td>Validation - top-1</td><td>▁▄▆██▇</td></tr><tr><td>Validation - top-10</td><td>▁▁█▇▆▇</td></tr><tr><td>Validation - top-5</td><td>▁▁▆█▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.48706</td></tr><tr><td>Train - top-1</td><td>0.65022</td></tr><tr><td>Train - top-10</td><td>0.89503</td></tr><tr><td>Train - top-5</td><td>0.84735</td></tr><tr><td>Validation - loss</td><td>1.90222</td></tr><tr><td>Validation - top-1</td><td>0.60517</td></tr><tr><td>Validation - top-10</td><td>0.85285</td></tr><tr><td>Validation - top-5</td><td>0.80298</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dutiful-sweep-7</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/jcgivjoq' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/jcgivjoq</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_082033-jcgivjoq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: svwo7xmb with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 577\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 575\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.253616715822834\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.000770141803485162\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_082922-svwo7xmb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/svwo7xmb' target=\"_blank\">stoic-sweep-8</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/svwo7xmb' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/svwo7xmb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.46     top-1: 0.65    top-5: 0.85    top-10: 0.90\n",
            "Eval -    loss: 1.90     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "I had to wait for Tom to finish.\n",
            "J'ai essayé de sortir avec Tom.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.46     top-1: 0.65    top-5: 0.85    top-10: 0.90\n",
            "Eval -    loss: 1.89     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "Watching TV is a passive activity.\n",
            "Le musée est un enfant gâté.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.47     top-1: 0.65    top-5: 0.85    top-10: 0.90\n",
            "Eval -    loss: 1.89     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "I wonder why women don't go bald.\n",
            "Je me demande pourquoi nous sommes morts.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.45     top-1: 0.65    top-5: 0.85    top-10: 0.90\n",
            "Eval -    loss: 1.89     top-1: 0.61    top-5: 0.81    top-10: 0.85\n",
            "I found the picture you were looking for.\n",
            "J'ai trouvé la photo que tu cherchais.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.44     top-1: 0.66    top-5: 0.85    top-10: 0.90\n",
            "Eval -    loss: 1.89     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "I don't have to apologize for what I said.\n",
            "Je n'ai pas peur d'aider Tom.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.44     top-1: 0.66    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.88     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "What are you reading?\n",
            "Que lisez-vous   ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▄▅▅▇▆█▃▄▅▅▆▇▇▄▅▅▅▆▆▂▃▄▅▆▆▇▃▄▄▄▆▆▁▄▄▄▅▆▆</td></tr><tr><td>Train - top-1</td><td>▆▄▃▄▂▃▁▆▄▄▄▃▂▂▅▄▄▄▄▄▇▅▄▄▃▄▂▅▅▅▅▄▃█▅▅▅▅▄▃</td></tr><tr><td>Train - top-10</td><td>▆▅▄▄▂▂▁▆▅▄▄▂▂▂▅▄▄▃▃▃█▅▄▃▃▃▁▆▅▅▅▃▃█▅▅▅▄▄▃</td></tr><tr><td>Train - top-5</td><td>▆▄▄▃▂▂▁▆▄▃▃▂▁▂▅▄▄▃▃▃▇▅▄▄▃▄▂▆▅▅▅▃▃█▅▅▅▄▄▃</td></tr><tr><td>Validation - loss</td><td>█▅▃▄▃▁</td></tr><tr><td>Validation - top-1</td><td>▁▅▅▇▇█</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▄▆█</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.43726</td></tr><tr><td>Train - top-1</td><td>0.65743</td></tr><tr><td>Train - top-10</td><td>0.9001</td></tr><tr><td>Train - top-5</td><td>0.85531</td></tr><tr><td>Validation - loss</td><td>1.88399</td></tr><tr><td>Validation - top-1</td><td>0.61054</td></tr><tr><td>Validation - top-10</td><td>0.85624</td></tr><tr><td>Validation - top-5</td><td>0.80666</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stoic-sweep-8</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/svwo7xmb' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/svwo7xmb</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_082922-svwo7xmb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gmkk8sv6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 422\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.19391275354122212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.000645164751201379\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_083757-gmkk8sv6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/gmkk8sv6' target=\"_blank\">still-sweep-9</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/gmkk8sv6' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/gmkk8sv6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.45     top-1: 0.66    top-5: 0.85    top-10: 0.90\n",
            "Eval -    loss: 1.89     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "He is absorbed in reading detective novels.\n",
            "Il se connaît la nouvelle institutrice.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.41     top-1: 0.66    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.89     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "Tom isn't joking.\n",
            "Tom ne plaisante pas.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.42     top-1: 0.66    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.89     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "This was a bad week. My train was late two days in a row.\n",
            "C'était un mauvais jour de Tokyo.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.42     top-1: 0.66    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.89     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "She advised him not to go.\n",
            "Elle lui a conseillé de ne pas y aller.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.44     top-1: 0.66    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.88     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "Tom and I are going to Boston for a conference.\n",
            "Tom et Marie reviennent en excuses pour quelques semaines.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.38     top-1: 0.67    top-5: 0.86    top-10: 0.91\n",
            "Eval -    loss: 1.88     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "The flowers wilted.\n",
            "Les fleurs.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▂▄▄▆▇▇█▃▄▅▆▇▇█▄▅▆▇▇█▁▄▄▆▇▇█▂▄▅▆▆▇▁▃▄▆▆▆▇</td></tr><tr><td>Train - top-1</td><td>▆▃▄▃▂▂▁▆▄▃▄▂▂▁▅▄▃▂▂▂█▄▄▃▂▂▂▆▄▃▃▃▃█▆▄▃▂▃▃</td></tr><tr><td>Train - top-10</td><td>█▅▅▄▂▂▁▆▅▃▃▂▃▂▆▄▃▃▂▁█▅▅▃▃▂▁▇▅▄▄▃▂█▆▅▄▃▃▃</td></tr><tr><td>Train - top-5</td><td>▇▄▄▃▂▁▁▆▅▄▃▂▃▁▆▄▃▂▂▁█▄▅▃▂▃▁▇▅▄▄▃▂█▅▅▃▃▃▃</td></tr><tr><td>Validation - loss</td><td>█▇▇▆▃▁</td></tr><tr><td>Validation - top-1</td><td>▁▂▃▄▇█</td></tr><tr><td>Validation - top-10</td><td>▁▃▃▆▄█</td></tr><tr><td>Validation - top-5</td><td>▁▂▄▆▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.37764</td></tr><tr><td>Train - top-1</td><td>0.66574</td></tr><tr><td>Train - top-10</td><td>0.907</td></tr><tr><td>Train - top-5</td><td>0.86241</td></tr><tr><td>Validation - loss</td><td>1.87724</td></tr><tr><td>Validation - top-1</td><td>0.61298</td></tr><tr><td>Validation - top-10</td><td>0.85788</td></tr><tr><td>Validation - top-5</td><td>0.80926</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">still-sweep-9</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/gmkk8sv6' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/gmkk8sv6</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_083757-gmkk8sv6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y125unxd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 402\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 144\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2394009804916092\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009383851854113872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_084636-y125unxd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/y125unxd' target=\"_blank\">deep-sweep-10</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/kob3h22b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/y125unxd' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/y125unxd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 6 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.40     top-1: 0.66    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.88     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "You're probably tired.\n",
            "Vous êtes probablement fatiguées.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.37     top-1: 0.67    top-5: 0.86    top-10: 0.91\n",
            "Eval -    loss: 1.88     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "We heard screaming outside.\n",
            "Nous avons déjeuné.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.37     top-1: 0.67    top-5: 0.86    top-10: 0.91\n",
            "Eval -    loss: 1.88     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "I want everything to be just right.\n",
            "Je veux que tout va bien.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.36     top-1: 0.67    top-5: 0.87    top-10: 0.91\n",
            "Eval -    loss: 1.88     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "They're right behind me.\n",
            "Elles sont juste derrière moi.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.36     top-1: 0.67    top-5: 0.86    top-10: 0.91\n",
            "Eval -    loss: 1.87     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "She didn't want him to die.\n",
            "Elle ne voulait pas qu'il mourût.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.36     top-1: 0.67    top-5: 0.87    top-10: 0.91\n",
            "Eval -    loss: 1.87     top-1: 0.61    top-5: 0.81    top-10: 0.86\n",
            "Try to get a good night's sleep.\n",
            "Essaie d'avoir un verre d'eau tôt.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▅▆▇▇██▃▅▅▇▇▇█▄▅▆▇▇█▃▄▆▆▆▇▇▃▄▅▇██▁▄▅▅▇▇▇</td></tr><tr><td>Train - top-1</td><td>▆▄▃▁▂▁▂▆▄▄▂▁▂▂▅▄▃▂▂▂▆▅▃▂▂▃▂▆▄▃▂▁▂█▅▄▄▂▃▂</td></tr><tr><td>Train - top-10</td><td>▆▅▄▂▂▂▁▆▅▄▂▂▂▁▆▅▃▃▂▂▆▅▄▃▃▃▃▆▅▄▃▂▂█▆▄▄▂▂▂</td></tr><tr><td>Train - top-5</td><td>▆▄▃▂▂▂▁▆▄▄▂▂▁▁▆▅▂▂▂▁▆▅▃▃▃▂▃▆▅▃▃▁▁█▆▄▄▂▃▂</td></tr><tr><td>Validation - loss</td><td>█▃▅▃▂▁</td></tr><tr><td>Validation - top-1</td><td>▁▂▁▁█▇</td></tr><tr><td>Validation - top-10</td><td>▄█▁▅▅▇</td></tr><tr><td>Validation - top-5</td><td>▂▃▁▅▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.35778</td></tr><tr><td>Train - top-1</td><td>0.66954</td></tr><tr><td>Train - top-10</td><td>0.91063</td></tr><tr><td>Train - top-5</td><td>0.8672</td></tr><tr><td>Validation - loss</td><td>1.87342</td></tr><tr><td>Validation - top-1</td><td>0.61462</td></tr><tr><td>Validation - top-10</td><td>0.85803</td></tr><tr><td>Validation - top-5</td><td>0.81055</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deep-sweep-10</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/y125unxd' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/y125unxd</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_084636-y125unxd/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(\"INF8225 - TP3/i736m3vi\", train, count=10) # Transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7FJ4uK683Zyp",
        "outputId": "7c690bb2-74c5-445b-d470-e00e78e8e853"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: swo5t39a with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 73\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.22992617160098855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006994117711121344\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayzeg\u001b[0m (\u001b[33m8225_team_\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_090909-swo5t39a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/swo5t39a' target=\"_blank\">silvery-sweep-5</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/swo5t39a' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/swo5t39a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 3.21     top-1: 0.43    top-5: 0.61    top-10: 0.67\n",
            "Eval -    loss: 3.10     top-1: 0.44    top-5: 0.62    top-10: 0.68\n",
            "We're talking about you.\n",
            "Nous sommes heureux.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.87     top-1: 0.47    top-5: 0.66    top-10: 0.72\n",
            "Eval -    loss: 2.76     top-1: 0.49    top-5: 0.67    top-10: 0.73\n",
            "I'm ready to try doing that.\n",
            "Je suis sûr de le faire ça.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.64     top-1: 0.50    top-5: 0.69    top-10: 0.75\n",
            "Eval -    loss: 2.51     top-1: 0.52    top-5: 0.71    top-10: 0.76\n",
            "I wonder how long we'll have to wait.\n",
            "Je me suis imaginé quelque chose.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.43     top-1: 0.54    top-5: 0.73    top-10: 0.78\n",
            "Eval -    loss: 2.31     top-1: 0.55    top-5: 0.74    top-10: 0.79\n",
            "Let's vote.\n",
            "Commençons.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.23     top-1: 0.56    top-5: 0.75    top-10: 0.81\n",
            "Eval -    loss: 2.16     top-1: 0.58    top-5: 0.76    top-10: 0.81\n",
            "The moon emerged from behind the cloud.\n",
            "Le sommet de la rivière.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.15     top-1: 0.58    top-5: 0.77    top-10: 0.82\n",
            "Eval -    loss: 2.04     top-1: 0.59    top-5: 0.78    top-10: 0.83\n",
            "We're currently experiencing some turbulence.\n",
            "Nous sommes en train de bois.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 2.02     top-1: 0.59    top-5: 0.78    top-10: 0.83\n",
            "Eval -    loss: 1.95     top-1: 0.61    top-5: 0.80    top-10: 0.84\n",
            "How dare you accuse me of lying!\n",
            "Comment oses-tu me réveiller.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.93     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "Eval -    loss: 1.87     top-1: 0.62    top-5: 0.81    top-10: 0.85\n",
            "What would you like?\n",
            "Que ferais-tu ?\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.88     top-1: 0.61    top-5: 0.81    top-10: 0.85\n",
            "Eval -    loss: 1.82     top-1: 0.63    top-5: 0.81    top-10: 0.86\n",
            "He showed courage in the face of great danger.\n",
            "Il a montré dans la classe.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.83     top-1: 0.62    top-5: 0.81    top-10: 0.86\n",
            "Eval -    loss: 1.77     top-1: 0.64    top-5: 0.82    top-10: 0.86\n",
            "There was a food fight in the cafeteria.\n",
            "Il y avait une nourriture.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.75     top-1: 0.63    top-5: 0.82    top-10: 0.87\n",
            "Eval -    loss: 1.72     top-1: 0.64    top-5: 0.83    top-10: 0.87\n",
            "Come back home.\n",
            "Reviens la maison.\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.73     top-1: 0.63    top-5: 0.83    top-10: 0.87\n",
            "Eval -    loss: 1.69     top-1: 0.65    top-5: 0.83    top-10: 0.87\n",
            "I'm not afraid of you anymore.\n",
            "Je n'ai plus peur de vous.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▆▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>Train - top-10</td><td>▁▃▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>Train - top-5</td><td>▁▃▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>Validation - loss</td><td>█▆▅▄▃▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▂▄▅▆▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▃▄▅▆▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▃▄▅▆▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.72659</td></tr><tr><td>Train - top-1</td><td>0.63356</td></tr><tr><td>Train - top-10</td><td>0.87179</td></tr><tr><td>Train - top-5</td><td>0.82759</td></tr><tr><td>Validation - loss</td><td>1.68834</td></tr><tr><td>Validation - top-1</td><td>0.64815</td></tr><tr><td>Validation - top-10</td><td>0.87183</td></tr><tr><td>Validation - top-5</td><td>0.83275</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">silvery-sweep-5</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/swo5t39a' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/swo5t39a</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_090909-swo5t39a/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x7xqxxu0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 700\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 289\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.13130109068684753\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009352356160543272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_091819-x7xqxxu0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/x7xqxxu0' target=\"_blank\">ethereal-sweep-6</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/x7xqxxu0' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/x7xqxxu0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.66     top-1: 0.65    top-5: 0.84    top-10: 0.88\n",
            "Eval -    loss: 1.65     top-1: 0.65    top-5: 0.84    top-10: 0.88\n",
            "It's completely dark.\n",
            "C'est complètement noir.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.65     top-1: 0.64    top-5: 0.84    top-10: 0.88\n",
            "Eval -    loss: 1.63     top-1: 0.66    top-5: 0.84    top-10: 0.88\n",
            "Have you already fed the dog?\n",
            "As-tu déjà nourri le chien ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.59     top-1: 0.66    top-5: 0.84    top-10: 0.89\n",
            "Eval -    loss: 1.60     top-1: 0.66    top-5: 0.84    top-10: 0.88\n",
            "Where did you learn to speak French?\n",
            "Où as-tu appris à parler français ?\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.57     top-1: 0.66    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.58     top-1: 0.66    top-5: 0.85    top-10: 0.88\n",
            "You broke the washing machine.\n",
            "Tu as enfreint la machine.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.60     top-1: 0.66    top-5: 0.85    top-10: 0.88\n",
            "Eval -    loss: 1.56     top-1: 0.67    top-5: 0.85    top-10: 0.89\n",
            "Tom turned right.\n",
            "Tom a tourné raison.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.51     top-1: 0.67    top-5: 0.86    top-10: 0.89\n",
            "Eval -    loss: 1.54     top-1: 0.67    top-5: 0.85    top-10: 0.89\n",
            "You don't speak French, do you?\n",
            "Tu ne parles pas français, si ?\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.50     top-1: 0.67    top-5: 0.86    top-10: 0.89\n",
            "Eval -    loss: 1.52     top-1: 0.68    top-5: 0.86    top-10: 0.89\n",
            "We're all mothers.\n",
            "Nous sommes toutes fausses.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.51     top-1: 0.67    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.51     top-1: 0.68    top-5: 0.86    top-10: 0.89\n",
            "They're bad.\n",
            "Ils sont mauvais.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.49     top-1: 0.67    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.50     top-1: 0.68    top-5: 0.86    top-10: 0.89\n",
            "I always said no.\n",
            "Je n'ai toujours rien dit.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.46     top-1: 0.67    top-5: 0.86    top-10: 0.90\n",
            "Eval -    loss: 1.48     top-1: 0.68    top-5: 0.86    top-10: 0.90\n",
            "Tom and Mary usually speak French to each other.\n",
            "Tom et Marie parlent généralement français.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.43     top-1: 0.68    top-5: 0.87    top-10: 0.90\n",
            "Eval -    loss: 1.47     top-1: 0.69    top-5: 0.86    top-10: 0.90\n",
            "I feel like I have to be there.\n",
            "J'ai l'impression de pouvoir être là.\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.42     top-1: 0.68    top-5: 0.87    top-10: 0.90\n",
            "Eval -    loss: 1.46     top-1: 0.69    top-5: 0.86    top-10: 0.90\n",
            "I'm as tired as tired can be.\n",
            "Je suis aussi fatigué que possible.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▆███▇▇▇▆▆▆▄▅▆▅▅▅▅▃▄▅▃▄▄▃▃▄▄▂▃▃▂▂▃▁▂▃▂▂▂▂</td></tr><tr><td>Train - top-1</td><td>▂▁▁▂▂▁▂▃▃▃▄▄▃▄▄▄▅▅▅▄▆▅▅▆▆▅▅▇▇▇▇▇▇█▇▇▇███</td></tr><tr><td>Train - top-10</td><td>▃▁▁▂▃▂▂▃▄▃▅▄▃▄▄▄▅▆▅▄▇▆▅▆▇▅▆▇▇▆█▇▆█▇▇▇██▇</td></tr><tr><td>Train - top-5</td><td>▃▁▁▂▂▂▂▃▃▃▅▄▃▄▄▄▅▆▅▄▆▅▅▆▇▅▆▇▇▆█▇▇█▇▇▇███</td></tr><tr><td>Validation - loss</td><td>█▇▆▅▅▄▃▃▂▂▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▂▃▃▄▅▆▆▇▇██</td></tr><tr><td>Validation - top-10</td><td>▁▂▃▄▄▅▆▆▇███</td></tr><tr><td>Validation - top-5</td><td>▁▂▃▄▄▅▆▆▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.4232</td></tr><tr><td>Train - top-1</td><td>0.67939</td></tr><tr><td>Train - top-10</td><td>0.90422</td></tr><tr><td>Train - top-5</td><td>0.86736</td></tr><tr><td>Validation - loss</td><td>1.46373</td></tr><tr><td>Validation - top-1</td><td>0.68617</td></tr><tr><td>Validation - top-10</td><td>0.89791</td></tr><tr><td>Validation - top-5</td><td>0.86383</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ethereal-sweep-6</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/x7xqxxu0' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/x7xqxxu0</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_091819-x7xqxxu0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xc4uydyh with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 651\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.07011145463813385\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006600273023362729\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_092730-xc4uydyh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xc4uydyh' target=\"_blank\">ethereal-sweep-7</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xc4uydyh' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xc4uydyh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.40     top-1: 0.69    top-5: 0.87    top-10: 0.91\n",
            "Eval -    loss: 1.45     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "May I sit next to you?\n",
            "Puis-je t'asseoir à côté ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.39     top-1: 0.68    top-5: 0.87    top-10: 0.91\n",
            "Eval -    loss: 1.45     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "She has nothing to do with that affair.\n",
            "Elle n'a rien à faire avec ça.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.38     top-1: 0.69    top-5: 0.87    top-10: 0.91\n",
            "Eval -    loss: 1.43     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Tom is upbeat.\n",
            "Tom est <unk>.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.42     top-1: 0.68    top-5: 0.87    top-10: 0.90\n",
            "Eval -    loss: 1.44     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "I quit.\n",
            "J'ai démissionné.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.34     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.42     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Tom and Mary were very busy.\n",
            "Tom et Marie étaient très occupée.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.36     top-1: 0.69    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.41     top-1: 0.70    top-5: 0.87    top-10: 0.90\n",
            "Did you clinch the deal?\n",
            "As-tu reçu les pieds ?\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.35     top-1: 0.69    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.40     top-1: 0.70    top-5: 0.87    top-10: 0.90\n",
            "This is silly.\n",
            "C'est bête.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.31     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.40     top-1: 0.70    top-5: 0.87    top-10: 0.91\n",
            "Why would you do that without telling us?\n",
            "Pourquoi ferais-tu ça sans nous dire ?\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.28     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.39     top-1: 0.70    top-5: 0.87    top-10: 0.91\n",
            "I know it won't be easy to do that.\n",
            "Je sais que ça ne sera pas facile à faire ça.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.30     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.38     top-1: 0.70    top-5: 0.87    top-10: 0.91\n",
            "Tom doesn't cook well.\n",
            "Tom ne parle pas bien.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.29     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.38     top-1: 0.70    top-5: 0.87    top-10: 0.91\n",
            "I think I've seen enough.\n",
            "Je pense que j'ai déjà vu.\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.29     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.38     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "I'll tell you only what you need to know.\n",
            "Je te dirai à ce que tu as besoin de savoir.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▄▇█▇▄▆▇▄▅▆▃▄▅█▄▅▅▃▃▄▁▅▄▂▃▃▃▂▃▃▂▂▄▁▁▃▃▂▂▃</td></tr><tr><td>Train - top-1</td><td>▃▂▁▂▃▂▃▄▄▃▅▄▅▂▅▄▃▅▆▆▇▄▆▆▅▆▆▇▆▇▆█▅▇█▇▇▇▇▇</td></tr><tr><td>Train - top-10</td><td>▆▃▁▃▄▄▃▄▄▃▆▅▄▂▅▄▄▆▅▅▇▄▅▇▇▆▆▇▆▇▇▇▅█▇▆▆▇▆▆</td></tr><tr><td>Train - top-5</td><td>▅▂▁▃▄▃▃▄▄▃▆▅▅▂▅▄▄▆▅▅▇▄▆▇▆▆▆▇▆▇▇▇▅██▆▇▇▇▆</td></tr><tr><td>Validation - loss</td><td>██▆▆▅▄▃▃▂▂▂▁</td></tr><tr><td>Validation - top-1</td><td>▁▁▃▂▃▅▅▆▇▇▇█</td></tr><tr><td>Validation - top-10</td><td>▁▂▃▃▄▄▆▆▇▇▇█</td></tr><tr><td>Validation - top-5</td><td>▁▂▃▃▄▅▆▇▇█▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.28716</td></tr><tr><td>Train - top-1</td><td>0.70235</td></tr><tr><td>Train - top-10</td><td>0.91854</td></tr><tr><td>Train - top-5</td><td>0.88329</td></tr><tr><td>Validation - loss</td><td>1.37801</td></tr><tr><td>Validation - top-1</td><td>0.70341</td></tr><tr><td>Validation - top-10</td><td>0.90736</td></tr><tr><td>Validation - top-5</td><td>0.87535</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ethereal-sweep-7</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xc4uydyh' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xc4uydyh</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_092730-xc4uydyh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5fih3buq with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 577\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 575\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.253616715822834\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.000770141803485162\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_093639-5fih3buq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/5fih3buq' target=\"_blank\">expert-sweep-8</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/5fih3buq' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/5fih3buq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.28     top-1: 0.71    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.37     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "Why does my dog hate Tom?\n",
            "Pourquoi mon chien   ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.29     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.37     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "I think it's time for a beer.\n",
            "Je pense qu'il est temps pour une bière.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.26     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.36     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Can I ask you for a favor?\n",
            "Puis-je vous demander une faveur ?\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.25     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.36     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Tom has decided to leave the company.\n",
            "Tom a décidé de partir de l'entreprise.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.28     top-1: 0.71    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.36     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "I've made a list of things I'd like to buy.\n",
            "J'ai fait une liste de choses dont j'aimerais acheter.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.23     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.35     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "This is all a conspiracy.\n",
            "C'est tout une conspiration.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.24     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.35     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "That's just what I need.\n",
            "C'est juste ce que j'ai besoin.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.23     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.35     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "You promised you'd stay.\n",
            "Tu avais promis que tu restes.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.22     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.34     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Where did you go?\n",
            "Où êtes-vous allées ?\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.25     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.33     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Tom smelled something.\n",
            "Tom sentait quelque chose.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.22     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.33     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "I'm not used to the heat.\n",
            "Je ne suis pas habituée à la chaleur.\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.20     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.33     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Why didn't you call me last night?\n",
            "Pourquoi n'as-tu pas appelée hier soir ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▄▆██▅█▆▅▆▆▄▆▇▇▅▅▇▅▅▆▂▄▅▃▃▅▆▃▅▆▃▃▅▁▃▄▅▄▄▅</td></tr><tr><td>Train - top-1</td><td>▃▁▁▁▃▁▂▂▃▃▄▂▂▃▃▄▂▄▄▃▆▄▄▅▅▄▃▅▄▄▆▅▄█▅▅▄▅▅▄</td></tr><tr><td>Train - top-10</td><td>▅▃▂▁▄▂▃▅▄▃▅▃▂▂▄▄▂▅▄▃▇▅▄▆▆▄▄▇▄▃▇▆▅█▆▅▄▆▅▄</td></tr><tr><td>Train - top-5</td><td>▄▂▁▁▃▁▃▃▃▃▅▃▂▂▃▄▁▄▄▃▇▅▄▆▆▄▄▆▄▄▆▅▅█▆▅▄▅▅▄</td></tr><tr><td>Validation - loss</td><td>█▇▅▆▅▄▄▃▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▁▄▃▄▅▅▅▆▇██</td></tr><tr><td>Validation - top-10</td><td>▁▃▃▄▄▄▅▅▆███</td></tr><tr><td>Validation - top-5</td><td>▁▂▃▃▄▄▅▄▆███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.20497</td></tr><tr><td>Train - top-1</td><td>0.71944</td></tr><tr><td>Train - top-10</td><td>0.92491</td></tr><tr><td>Train - top-5</td><td>0.89224</td></tr><tr><td>Validation - loss</td><td>1.33232</td></tr><tr><td>Validation - top-1</td><td>0.71276</td></tr><tr><td>Validation - top-10</td><td>0.91254</td></tr><tr><td>Validation - top-5</td><td>0.88194</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">expert-sweep-8</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/5fih3buq' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/5fih3buq</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_093639-5fih3buq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xa4xttau with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 422\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.19391275354122212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.000645164751201379\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_094550-xa4xttau</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xa4xttau' target=\"_blank\">crimson-sweep-9</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xa4xttau' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xa4xttau</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.22     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.33     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "She hates him.\n",
            "Elle lui déteste.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.21     top-1: 0.72    top-5: 0.89    top-10: 0.93\n",
            "Eval -    loss: 1.32     top-1: 0.72    top-5: 0.88    top-10: 0.91\n",
            "I'm so thirsty.\n",
            "J'ai tellement soif.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.19     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.32     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Do your parents leave you home alone?\n",
            "Vos parents me quittez la maison ?\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.21     top-1: 0.72    top-5: 0.89    top-10: 0.93\n",
            "Eval -    loss: 1.32     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Don't be such a cheapskate.\n",
            "Ne sois pas si mignonne.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.21     top-1: 0.72    top-5: 0.89    top-10: 0.93\n",
            "Eval -    loss: 1.32     top-1: 0.72    top-5: 0.88    top-10: 0.91\n",
            "Do you think you could help me do that?\n",
            "Pensez-vous que tu pourrais m'aider ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.19     top-1: 0.72    top-5: 0.89    top-10: 0.93\n",
            "Eval -    loss: 1.32     top-1: 0.72    top-5: 0.88    top-10: 0.91\n",
            "Tom speaks French much better than Mary does.\n",
            "Tom parle beaucoup mieux que Marie.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.20     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.32     top-1: 0.72    top-5: 0.88    top-10: 0.91\n",
            "She handed me the letter without saying anything.\n",
            "Elle m'a tendu la lettre sans dire.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.16     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.31     top-1: 0.72    top-5: 0.88    top-10: 0.91\n",
            "I can give you something for your pain.\n",
            "Je peux te donner quelque chose pour votre douleur.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.17     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.31     top-1: 0.72    top-5: 0.88    top-10: 0.91\n",
            "The competition has become fierce.\n",
            "La concurrence a été en vain.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.18     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.31     top-1: 0.72    top-5: 0.89    top-10: 0.91\n",
            "The cat is sitting on the table.\n",
            "Le chat est assis sur la table.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.18     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.31     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Doesn't that car look familiar?\n",
            "N'interromps-t-il pas cette voiture   ?\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.15     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.30     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "What could possibly go wrong?\n",
            "Que pourraient arriver ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▄▆▆█▄▆▇▄▆▆▄▅▇▇▅▅▇▃▄▇▃▄▅▃▄▆▅▄▄▅▂▄▅▁▄▅▆▃▄▅</td></tr><tr><td>Train - top-1</td><td>▄▂▃▁▅▃▂▆▂▃▄▃▁▁▅▄▃▅▅▂▆▆▃▆▅▃▄▅▆▅▇▆▅█▅▆▃▆▅▄</td></tr><tr><td>Train - top-10</td><td>▅▃▃▁▅▃▂▅▃▃▅▄▁▂▄▄▂▅▄▂▆▄▄▅▅▄▃▆▅▃▇▅▄█▅▄▃▆▅▃</td></tr><tr><td>Train - top-5</td><td>▅▄▄▁▅▃▂▅▃▃▅▄▂▂▄▄▂▆▅▃▆▅▄▆▅▄▅▆▅▄▆▅▄█▆▅▃▆▅▄</td></tr><tr><td>Validation - loss</td><td>█▆▆▅▅▅▄▃▃▂▂▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▃▃▅▅▅▆▇▇█▇</td></tr><tr><td>Validation - top-10</td><td>▁▂▁▄▅▅▅▅▅▇▇█</td></tr><tr><td>Validation - top-5</td><td>▁▃▂▄▅▅▅▅▆▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.15307</td></tr><tr><td>Train - top-1</td><td>0.72938</td></tr><tr><td>Train - top-10</td><td>0.93084</td></tr><tr><td>Train - top-5</td><td>0.90008</td></tr><tr><td>Validation - loss</td><td>1.30386</td></tr><tr><td>Validation - top-1</td><td>0.71872</td></tr><tr><td>Validation - top-10</td><td>0.91533</td></tr><tr><td>Validation - top-5</td><td>0.88612</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">crimson-sweep-9</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xa4xttau' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xa4xttau</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_094550-xa4xttau/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yi8cg81f with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 402\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 144\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2394009804916092\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009383851854113872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_095501-yi8cg81f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/yi8cg81f' target=\"_blank\">autumn-sweep-10</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/yi8cg81f' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/yi8cg81f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.16     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.30     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Do we have a deal here?\n",
            "Avons-nous une décision ici ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.14     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.30     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "The whole soccer team was on cloud nine after winning the championship.\n",
            "Toute la football était le fruit au regard après le championnat.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.15     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.30     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "How arrogant!\n",
            "Comme c'est arrogant !\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.14     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "I'm sure this is only temporary.\n",
            "Je suis certaine que c'est seulement temporaire.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.14     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Which one of them was it?\n",
            "Laquelle d'entre eux était ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.17     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Where are they sending us?\n",
            "Où nous téléphone nous a embrassé ?\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.15     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "I saw my sister tear up the letter.\n",
            "J'ai vu ma sœur à la lettre.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.13     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "This mouse was killed by my cat.\n",
            "Cette souris a été tué par mon chat.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.13     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "None of the money is yours.\n",
            "Rien de l'argent n'est le tien.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.11     top-1: 0.73    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "The floor has to be scrubbed.\n",
            "Le sol doit être récuré.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.11     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "My mother put a large vase on the shelf.\n",
            "Ma mère a mis un grand vase sur l'étagère.\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.12     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "I was wrong about that.\n",
            "J'ai eu tort à ça.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▅▆█▅▆▇▃▅█▁▄▆▇▄▅▇▄▄▅▂▅▄▂▃▆▆▂▄▅▁▅▅▁▃▅▅▄▃▅</td></tr><tr><td>Train - top-1</td><td>▆▃▃▂▂▃▃▅▄▁█▅▂▂▄▃▃▅▅▄▆▄▆▆▆▂▃█▅▄█▄▅█▇▄▄▅▆▄</td></tr><tr><td>Train - top-10</td><td>▆▄▃▁▄▃▂▆▄▁▇▅▃▃▄▄▂▅▅▃▆▄▄▇▆▃▃▆▅▃▇▄▄█▅▃▄▅▆▄</td></tr><tr><td>Train - top-5</td><td>▆▄▃▁▄▄▃▅▅▂▇▅▄▃▅▅▂▅▅▄▆▄▄▇▆▃▃▆▅▄▇▄▄█▆▄▄▅▆▃</td></tr><tr><td>Validation - loss</td><td>█▆▇▄▄▄▄▄▃▂▁▂</td></tr><tr><td>Validation - top-1</td><td>▂▁▄▃▅▄▇▆▇▆██</td></tr><tr><td>Validation - top-10</td><td>▁▂▃▄▅▄▅▅█▆▇▇</td></tr><tr><td>Validation - top-5</td><td>▁▂▂▃▄▅▅▆▆▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.12043</td></tr><tr><td>Train - top-1</td><td>0.73397</td></tr><tr><td>Train - top-10</td><td>0.93225</td></tr><tr><td>Train - top-5</td><td>0.90147</td></tr><tr><td>Validation - loss</td><td>1.28651</td></tr><tr><td>Validation - top-1</td><td>0.7235</td></tr><tr><td>Validation - top-10</td><td>0.91741</td></tr><tr><td>Validation - top-5</td><td>0.88893</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">autumn-sweep-10</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/yi8cg81f' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/yi8cg81f</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_095501-yi8cg81f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ipjf6aiz with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 451\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2263208499636016\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0006378170606560637\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_100422-ipjf6aiz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ipjf6aiz' target=\"_blank\">usual-sweep-11</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ipjf6aiz' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ipjf6aiz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.13     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Would you like to travel to the United States?\n",
            "Aimerais-tu voyager aux États-Unis ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.12     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "We trust him.\n",
            "Nous l'avons confiance.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.09     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.28     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I prefer reading books to watching television.\n",
            "Je préfère lire des livres à regarder la télévision.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.12     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.28     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "I thought Tom might want to do that today.\n",
            "Je pensais que Tom pourrait faire cela aujourd'hui.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.09     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.28     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Don't approach the dog.\n",
            "Ne vous approchez pas.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.07     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.28     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "You're the richest man I know.\n",
            "Vous êtes le plus riche que je connaisse.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.10     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "Eval -    loss: 1.28     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "You are fabulous.\n",
            "Tu es fabuleux.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.08     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.29     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Help yourself to these cookies.\n",
            "Aidez-vous à ces biscuits.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.10     top-1: 0.73    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.28     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "He solved all those problems with ease.\n",
            "Il a résolu ces problèmes avec lui.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.09     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.28     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I lay on my bed.\n",
            "J'étais étendu sur mon lit.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.09     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.28     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Tom robbed a bank in Boston.\n",
            "Tom a volé une banque à Boston.\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.08     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I'm not always so lenient.\n",
            "Je ne suis pas toujours aussi excité.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▄▅█▄▆▇▃▅▆▂▄▇▇▄▆▅▃▅▆▂▅▅▁▄▄▅▂▄▆▂▅▆▁▃▅▆▄▃▄</td></tr><tr><td>Train - top-1</td><td>▆▄▅▁▄▃▂▅▄▅▇▅▃▂▄▃▄▅▅▄▇▄▅█▄▆▅█▆▃▇▅▃▇▇▅▅▄▆▅</td></tr><tr><td>Train - top-10</td><td>▅▅▃▁▅▃▂▆▄▃▇▅▂▂▅▃▃▆▄▃▇▄▃▇▅▄▄▇▄▃▆▄▃█▆▄▃▅▅▅</td></tr><tr><td>Train - top-5</td><td>▆▄▄▁▅▂▂▅▄▂▇▄▂▂▅▃▃▆▄▂▇▄▃█▅▅▄▇▅▃▆▃▄█▆▄▃▅▆▅</td></tr><tr><td>Validation - loss</td><td>██▅▇▇▅▄█▂▄▃▁</td></tr><tr><td>Validation - top-1</td><td>▁▁▅▁▃▃▅▃▄▅▅█</td></tr><tr><td>Validation - top-10</td><td>▂▁▃▃▂▂▂▆▄▂▆█</td></tr><tr><td>Validation - top-5</td><td>▂▁▂▄▄▃▃▅▅▃▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.07839</td></tr><tr><td>Train - top-1</td><td>0.73806</td></tr><tr><td>Train - top-10</td><td>0.93647</td></tr><tr><td>Train - top-5</td><td>0.90798</td></tr><tr><td>Validation - loss</td><td>1.27408</td></tr><tr><td>Validation - top-1</td><td>0.72695</td></tr><tr><td>Validation - top-10</td><td>0.91922</td></tr><tr><td>Validation - top-5</td><td>0.89065</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">usual-sweep-11</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ipjf6aiz' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ipjf6aiz</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_100422-ipjf6aiz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xqadt05t with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 908\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.12753689232983317\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008649158741300643\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_101331-xqadt05t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xqadt05t' target=\"_blank\">likely-sweep-3</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xqadt05t' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xqadt05t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.10     top-1: 0.73    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.28     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Do you know when she will come?\n",
            "Sais-tu quand elle viendra ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.06     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Maybe I need a new assistant.\n",
            "Peut-être ai-je besoin d'un nouvel assistant.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.08     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "If I'd known, I would've told you.\n",
            "Si j'avais su, je t'aurais dit.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.07     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Tom can speak French better than you.\n",
            "Tom parle mieux le français que toi.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.08     top-1: 0.73    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Have you ever been on TV?\n",
            "As-tu jamais été directement à la télé ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.07     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.28     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "It's bad weather, to be sure, but we've seen worse.\n",
            "C'est mauvais temps, mais nous sommes certains.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.05     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "You really don't get it, do you?\n",
            "Tu ne comprends vraiment pas, n'est-ce pas ?\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.08     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "What color are they?\n",
            "Quelle couleur sont-ils ?\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.06     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I don't care why Tom did it. I'm just glad he did it.\n",
            "Je me fiche de Tom.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.09     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "He will travel abroad next year.\n",
            "Il va voyager à l'année prochaine.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.07     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Did you remember to buy bread?\n",
            "Avez-vous pensé à acheter de pain ?\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.06     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Can't you tell us anything?\n",
            "Ne peux-tu pas nous dire ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▃▄▅█▃▅▆▄▅▅▂▄▅▆▄▅▆▃▄▅▂▅▅▂▃▆▇▃▇▄▂▅▅▁▄▅▆▃▄▆</td></tr><tr><td>Train - top-1</td><td>▆▅▄▁▆▅▂▅▄▄▆▅▄▃▄▃▃▆▅▅▇▄▅▅▅▃▂▆▃▆▇▃▄█▅▄▃▆▆▄</td></tr><tr><td>Train - top-10</td><td>▆▄▃▁▆▄▂▆▄▄▇▅▃▂▄▄▃▆▄▄▇▄▃▇▆▃▁▅▂▅▆▃▃█▅▄▂▅▄▃</td></tr><tr><td>Train - top-5</td><td>▆▄▄▁▇▄▂▅▄▃▇▅▃▂▄▄▃▆▄▄▇▄▃▆▆▃▁▆▂▅▇▃▃█▅▃▂▅▅▄</td></tr><tr><td>Validation - loss</td><td>▇▆▆▃▄█▅▄▃▃▁▁</td></tr><tr><td>Validation - top-1</td><td>▂▁▅▇▅▄▃▆█▄▇▇</td></tr><tr><td>Validation - top-10</td><td>▁▂▁█▆▄▆▆▆▅██</td></tr><tr><td>Validation - top-5</td><td>▁▁▁▇▄▃▆▃▆▆█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.06358</td></tr><tr><td>Train - top-1</td><td>0.7438</td></tr><tr><td>Train - top-10</td><td>0.93882</td></tr><tr><td>Train - top-5</td><td>0.90913</td></tr><tr><td>Validation - loss</td><td>1.2639</td></tr><tr><td>Validation - top-1</td><td>0.72841</td></tr><tr><td>Validation - top-10</td><td>0.91993</td></tr><tr><td>Validation - top-5</td><td>0.89153</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">likely-sweep-3</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xqadt05t' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/xqadt05t</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_101331-xqadt05t/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2ceuhwui with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 370\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.27825276330947457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009833031838853794\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_102241-2ceuhwui</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/2ceuhwui' target=\"_blank\">proud-sweep-4</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/2ceuhwui' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/2ceuhwui</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.05     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "As soon as I have it, I'll forward it to you.\n",
            "Dès que je l'ai hâte de vous.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.07     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "We talked about what we could do.\n",
            "Nous avons parlé de ce que nous pourrions faire.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.05     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I think Tom is here.\n",
            "Je pense que Tom est là.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.03     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "What year were you born?\n",
            "En quelle année es-tu né ?\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.06     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Don't say a word to anyone.\n",
            "Ne dites pas un mot à qui que ce soit.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.05     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "The doctor says she suffers from rheumatism.\n",
            "Le docteur dit qu'elle souffre d'accepter.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.04     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "They crushed all resistance.\n",
            "Ils ont profité de question.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.06     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "If you want one, you'll have to find your own.\n",
            "Si vous voulez un, tu auras à trouver votre propre propre.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.04     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I'm just telling you the truth.\n",
            "Je sais seulement que vous dites la vérité.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.04     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.27     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I didn't feel like going.\n",
            "Je n'avais pas envie d'y aller.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.06     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Are you still buying lottery tickets?\n",
            "Êtes-vous toujours en avance ?\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.04     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "This is the reason why he did it.\n",
            "C'est la raison pour ça qu'il l'a fait.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▂▅▆▇▄▆▇▅▆▆▁▅▆▆▄▇▆▃▅▇▂▅█▁▄▅█▄▅▆▂▅▆▁▃▆█▄▆▆</td></tr><tr><td>Train - top-1</td><td>▆▂▃▃▅▄▂▃▃▄▇▄▄▄▄▂▄▆▅▃▇▅▁█▅▄▄▆▄▄▇▄▃█▆▅▁▅▄▅</td></tr><tr><td>Train - top-10</td><td>▇▄▃▂▅▄▂▄▃▃▇▄▃▂▅▂▃▅▄▂▆▅▃█▅▄▂▅▄▂▆▅▃▇▆▄▁▅▂▃</td></tr><tr><td>Train - top-5</td><td>█▃▃▁▄▃▂▄▂▃█▄▃▂▅▁▄▆▄▁▇▅▂█▆▄▁▅▄▂▆▅▄▇▆▄▁▅▃▃</td></tr><tr><td>Validation - loss</td><td>▆▇▇▅▄▅▅▃▁█▁▂</td></tr><tr><td>Validation - top-1</td><td>▅▆▄▆█▄▁▇▇▅█▆</td></tr><tr><td>Validation - top-10</td><td>▁▄▃▄▂▄▄▃▇█▇▆</td></tr><tr><td>Validation - top-5</td><td>▃▃▂▃▁▆▃▄▆▆█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.04352</td></tr><tr><td>Train - top-1</td><td>0.74906</td></tr><tr><td>Train - top-10</td><td>0.94088</td></tr><tr><td>Train - top-5</td><td>0.91426</td></tr><tr><td>Validation - loss</td><td>1.26101</td></tr><tr><td>Validation - top-1</td><td>0.72967</td></tr><tr><td>Validation - top-10</td><td>0.9203</td></tr><tr><td>Validation - top-5</td><td>0.89253</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">proud-sweep-4</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/2ceuhwui' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/2ceuhwui</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_102241-2ceuhwui/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ncq2yi3f with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_embedding: 762\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim_hidden: 605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1626407766577888\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008301226893111945\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_heads: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230410_103151-ncq2yi3f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ncq2yi3f' target=\"_blank\">sage-sweep-12</a></strong> to <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/sweeps/i736m3vi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ncq2yi3f' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ncq2yi3f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 12 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.04     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I hear something.\n",
            "J'entends quelque chose.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.01     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "All the desk drawers were empty.\n",
            "Tous les tiroirs les tiroirs.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.03     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "He is playing outdoors.\n",
            "Il joue dehors.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.02     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I'd like to live here.\n",
            "J'aimerais vivre ici.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.02     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "He is influential.\n",
            "Il est influent.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.04     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I'm deaf.\n",
            "Je suis sourd.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.03     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "I've got it all sorted.\n",
            "Je l'ai tout.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.03     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "You're no longer welcome in my house.\n",
            "Vous n'êtes plus la bienvenue chez moi.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.04     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "We'll stop them.\n",
            "Nous nous aiderons.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.03     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "That boy speaks as if he were an adult.\n",
            "Ce garçon parle comme s'il y avait un adulte.\n",
            "\n",
            "Epoch 11\n",
            "Train -   loss: 1.03     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Get me some water.\n",
            "Fais-moi de l'eau !\n",
            "\n",
            "Epoch 12\n",
            "Train -   loss: 1.03     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.26     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "How did you hear about us?\n",
            "Comment vous avez-vous entendu parler ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>▂▄▆▇▃▆▇▄▄▆▁▄▇▆▄▅█▂▅▆▂▅▆▂▃▅▇▁▆▇▃▃▄▂▄▅▇▃▅▇</td></tr><tr><td>Train - top-1</td><td>█▅▃▃▆▃▂▅█▄█▅▄▃▅▅▁▆▅▂▇▄▄█▆▅▄█▅▃█▆▇▇▆▅▃▇▄▄</td></tr><tr><td>Train - top-10</td><td>█▅▃▁▆▃▂▅▅▃█▄▂▄▅▅▁▆▄▃▇▄▄▇▆▄▁█▃▂▇▅▅▇▆▄▂▆▄▂</td></tr><tr><td>Train - top-5</td><td>█▅▃▁▆▃▃▆▅▃█▅▃▃▅▄▁▆▄▃▆▄▄▇▇▄▂█▃▂▇▆▆▇▆▄▂▆▄▂</td></tr><tr><td>Validation - loss</td><td>▅▅▄▃█▃▇▅▃▅▁▂</td></tr><tr><td>Validation - top-1</td><td>▇▄▄▃▂▁█▁▇▇█▃</td></tr><tr><td>Validation - top-10</td><td>▅▄▃▆▁▅█▆▄▅▅▆</td></tr><tr><td>Validation - top-5</td><td>▄▄▆▅▁▇██▇▅▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.02868</td></tr><tr><td>Train - top-1</td><td>0.74743</td></tr><tr><td>Train - top-10</td><td>0.94158</td></tr><tr><td>Train - top-5</td><td>0.91555</td></tr><tr><td>Validation - loss</td><td>1.25776</td></tr><tr><td>Validation - top-1</td><td>0.73075</td></tr><tr><td>Validation - top-10</td><td>0.92067</td></tr><tr><td>Validation - top-5</td><td>0.89303</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sage-sweep-12</strong> at: <a href='https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ncq2yi3f' target=\"_blank\">https://wandb.ai/8225_team_/INF8225%20-%20TP3/runs/ncq2yi3f</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230410_103151-ncq2yi3f/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}